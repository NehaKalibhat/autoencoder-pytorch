tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,
        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,
        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,
        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,
        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,
        2, 0, 2, 7, 1, 8, 6, 4])
[20.0, 16.0, 12.8, 10.240000000000002, 8.192, 6.5536, 5.2428799999999995, 4.194304, 3.3554431999999994, 2.68435456, 2.1474836479999992, 1.7179869183999985, 1.3743895347199981, 1.099511627775999, 0.8796093022207998, 0.7036874417766399, 0.5629499534213125, 0.45035996273705053, 0.3602879701896399, 0.28823037615171077, 0.23058430092136747]
Weight fractions: [20.0, 36.0, 48.8, 59.04, 67.232, 73.7856, 79.02848, 83.222784, 86.5782272, 89.26258176, 91.41006540800001, 93.12805232640001, 94.50244186112, 95.601953488896, 96.4815627911168, 97.18525023289344, 97.74820018631475, 98.1985601490518, 98.55884811924145, 98.84707849539316]
***************Iterative Pruning started. Number of iterations: 20 *****************
Running pruning iteration 0
************pruning 20.0 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 2 / 1152  0.001736111111111111
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 34348 / 131072  0.262054443359375
encoder.2.bias : 8 / 64  0.125
encoder.4.weight : 743 / 8192  0.0906982421875
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 159 / 4608  0.034505208333333336
encoder.6.bias : 1 / 16  0.0625
encoder.9.weight : 9 / 1152  0.0078125
encoder.9.bias : 0 / 8  0.0
decoder.0.weight : 840 / 9216  0.09114583333333333
decoder.0.bias : 0 / 128  0.0
decoder.2.weight : 3493 / 32768  0.106597900390625
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 962 / 8192  0.117431640625
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 1372 / 12800  0.1071875
decoder.6.bias : 0 / 16  0.0
decoder.8.weight : 4 / 64  0.0625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 41941/209705 = 0.2
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.3835
epoch [2/100], loss:0.3882
epoch [3/100], loss:0.4132
epoch [4/100], loss:0.4011
epoch [5/100], loss:0.4140
epoch [6/100], loss:0.4345
epoch [7/100], loss:0.4107
epoch [8/100], loss:0.3923
epoch [9/100], loss:0.4064
epoch [10/100], loss:0.1345
epoch [11/100], loss:0.0767
epoch [12/100], loss:0.0640
epoch [13/100], loss:0.0552
epoch [14/100], loss:0.0483
epoch [15/100], loss:0.0394
epoch [16/100], loss:0.0426
epoch [17/100], loss:0.0379
epoch [18/100], loss:0.0355
epoch [19/100], loss:0.0367
epoch [20/100], loss:0.0335
epoch [21/100], loss:0.0328
epoch [22/100], loss:0.0343
epoch [23/100], loss:0.0336
epoch [24/100], loss:0.0310
epoch [25/100], loss:0.0322
epoch [26/100], loss:0.0298
epoch [27/100], loss:0.0280
epoch [28/100], loss:0.0285
epoch [29/100], loss:0.0289
epoch [30/100], loss:0.0269
epoch [31/100], loss:0.0346
epoch [32/100], loss:0.0262
epoch [33/100], loss:0.0283
epoch [34/100], loss:0.0270
epoch [35/100], loss:0.0245
epoch [36/100], loss:0.0288
epoch [37/100], loss:0.0282
epoch [38/100], loss:0.0245
epoch [39/100], loss:0.0271
epoch [40/100], loss:0.0246
epoch [41/100], loss:0.0242
epoch [42/100], loss:0.0252
epoch [43/100], loss:0.0209
epoch [44/100], loss:0.0227
epoch [45/100], loss:0.0201
epoch [46/100], loss:0.0210
epoch [47/100], loss:0.0201
epoch [48/100], loss:0.0224
epoch [49/100], loss:0.0203
epoch [50/100], loss:0.0246
epoch [51/100], loss:0.0236
epoch [52/100], loss:0.0233
epoch [53/100], loss:0.0229
epoch [54/100], loss:0.0197
epoch [55/100], loss:0.0223
epoch [56/100], loss:0.0226
epoch [57/100], loss:0.0209
epoch [58/100], loss:0.0222
epoch [59/100], loss:0.0210
epoch [60/100], loss:0.0204
epoch [61/100], loss:0.0202
epoch [62/100], loss:0.0212
epoch [63/100], loss:0.0201
epoch [64/100], loss:0.0191
epoch [65/100], loss:0.0198
epoch [66/100], loss:0.0183
epoch [67/100], loss:0.0201
epoch [68/100], loss:0.0199
epoch [69/100], loss:0.0201
epoch [70/100], loss:0.0204
epoch [71/100], loss:0.0199
epoch [72/100], loss:0.0196
epoch [73/100], loss:0.0191
epoch [74/100], loss:0.0188
epoch [75/100], loss:0.0191
epoch [76/100], loss:0.0207
epoch [77/100], loss:0.0206
epoch [78/100], loss:0.0212
epoch [79/100], loss:0.0180
epoch [80/100], loss:0.0210
epoch [81/100], loss:0.0177
epoch [82/100], loss:0.0191
epoch [83/100], loss:0.0208
epoch [84/100], loss:0.0212
epoch [85/100], loss:0.0171
epoch [86/100], loss:0.0187
epoch [87/100], loss:0.0193
epoch [88/100], loss:0.0171
epoch [89/100], loss:0.0177
epoch [90/100], loss:0.0191
epoch [91/100], loss:0.0205
epoch [92/100], loss:0.0192
epoch [93/100], loss:0.0208
epoch [94/100], loss:0.0177
epoch [95/100], loss:0.0208
epoch [96/100], loss:0.0206
epoch [97/100], loss:0.0184
epoch [98/100], loss:0.0193
epoch [99/100], loss:0.0201
epoch [100/100], loss:0.0188
Running pruning iteration 1
************pruning 36.0 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 2 / 1152  0.001736111111111111
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 61290 / 131072  0.4676055908203125
encoder.2.bias : 14 / 64  0.21875
encoder.4.weight : 1770 / 8192  0.216064453125
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 384 / 4608  0.08333333333333333
encoder.6.bias : 1 / 16  0.0625
encoder.9.weight : 44 / 1152  0.03819444444444445
encoder.9.bias : 0 / 8  0.0
decoder.0.weight : 1675 / 9216  0.18174913194444445
decoder.0.bias : 1 / 128  0.0078125
decoder.2.weight : 6762 / 32768  0.20635986328125
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 1629 / 8192  0.1988525390625
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 1913 / 12800  0.149453125
decoder.6.bias : 0 / 16  0.0
decoder.8.weight : 9 / 64  0.140625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 75494/209705 = 0.3600009537207029
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.3967
epoch [2/100], loss:0.3963
epoch [3/100], loss:0.4219
epoch [4/100], loss:0.3869
epoch [5/100], loss:0.4079
epoch [6/100], loss:0.4385
epoch [7/100], loss:0.2129
epoch [8/100], loss:0.1255
epoch [9/100], loss:0.1061
epoch [10/100], loss:0.0914
epoch [11/100], loss:0.0904
epoch [12/100], loss:0.0885
epoch [13/100], loss:0.0794
epoch [14/100], loss:0.0874
epoch [15/100], loss:0.0723
epoch [16/100], loss:0.0742
epoch [17/100], loss:0.0701
epoch [18/100], loss:0.0741
epoch [19/100], loss:0.0756
epoch [20/100], loss:0.0720
epoch [21/100], loss:0.0739
epoch [22/100], loss:0.0708
epoch [23/100], loss:0.0691
epoch [24/100], loss:0.0669
epoch [25/100], loss:0.0718
epoch [26/100], loss:0.0615
epoch [27/100], loss:0.0689
epoch [28/100], loss:0.0652
epoch [29/100], loss:0.0687
epoch [30/100], loss:0.0681
epoch [31/100], loss:0.0591
epoch [32/100], loss:0.0656
epoch [33/100], loss:0.0554
epoch [34/100], loss:0.0722
epoch [35/100], loss:0.0638
epoch [36/100], loss:0.0629
epoch [37/100], loss:0.0632
epoch [38/100], loss:0.0661
epoch [39/100], loss:0.0696
epoch [40/100], loss:0.0577
epoch [41/100], loss:0.0627
epoch [42/100], loss:0.0620
epoch [43/100], loss:0.0524
epoch [44/100], loss:0.0621
epoch [45/100], loss:0.0661
epoch [46/100], loss:0.0601
epoch [47/100], loss:0.0533
epoch [48/100], loss:0.0627
epoch [49/100], loss:0.0579
epoch [50/100], loss:0.0515
epoch [51/100], loss:0.0608
epoch [52/100], loss:0.0600
epoch [53/100], loss:0.0532
epoch [54/100], loss:0.0554
epoch [55/100], loss:0.0565
epoch [56/100], loss:0.0548
epoch [57/100], loss:0.0536
epoch [58/100], loss:0.0500
epoch [59/100], loss:0.0494
epoch [60/100], loss:0.0507
epoch [61/100], loss:0.0573
epoch [62/100], loss:0.0492
epoch [63/100], loss:0.0517
epoch [64/100], loss:0.0513
epoch [65/100], loss:0.0523
epoch [66/100], loss:0.0525
epoch [67/100], loss:0.0487
epoch [68/100], loss:0.0439
epoch [69/100], loss:0.0491
epoch [70/100], loss:0.0477
epoch [71/100], loss:0.0477
epoch [72/100], loss:0.0505
epoch [73/100], loss:0.0478
epoch [74/100], loss:0.0485
epoch [75/100], loss:0.0526
epoch [76/100], loss:0.0450
epoch [77/100], loss:0.0451
epoch [78/100], loss:0.0423
epoch [79/100], loss:0.0454
epoch [80/100], loss:0.0490
epoch [81/100], loss:0.0493
epoch [82/100], loss:0.0491
epoch [83/100], loss:0.0426
epoch [84/100], loss:0.0431
epoch [85/100], loss:0.0479
epoch [86/100], loss:0.0496
epoch [87/100], loss:0.0441
epoch [88/100], loss:0.0478
epoch [89/100], loss:0.0500
epoch [90/100], loss:0.0470
epoch [91/100], loss:0.0478
epoch [92/100], loss:0.0470
epoch [93/100], loss:0.0520
epoch [94/100], loss:0.0424
epoch [95/100], loss:0.0451
epoch [96/100], loss:0.0429
epoch [97/100], loss:0.0499
epoch [98/100], loss:0.0529
epoch [99/100], loss:0.0450
epoch [100/100], loss:0.0470
Running pruning iteration 2
************pruning 48.8 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 2 / 1152  0.001736111111111111
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 77654 / 131072  0.5924530029296875
encoder.2.bias : 18 / 64  0.28125
encoder.4.weight : 2464 / 8192  0.30078125
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 1000 / 4608  0.2170138888888889
encoder.6.bias : 2 / 16  0.125
encoder.9.weight : 213 / 1152  0.18489583333333334
encoder.9.bias : 3 / 8  0.375
decoder.0.weight : 4238 / 9216  0.4598524305555556
decoder.0.bias : 3 / 128  0.0234375
decoder.2.weight : 10543 / 32768  0.321746826171875
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 2663 / 8192  0.3250732421875
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 3522 / 12800  0.27515625
decoder.6.bias : 0 / 16  0.0
decoder.8.weight : 11 / 64  0.171875
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 102336/209705 = 0.4879998092558594
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.4227
epoch [2/100], loss:0.3929
epoch [3/100], loss:0.4093
epoch [4/100], loss:0.4030
epoch [5/100], loss:0.4086
epoch [6/100], loss:0.0958
epoch [7/100], loss:0.0726
epoch [8/100], loss:0.0567
epoch [9/100], loss:0.0503
epoch [10/100], loss:0.0459
epoch [11/100], loss:0.0442
epoch [12/100], loss:0.0435
epoch [13/100], loss:0.0394
epoch [14/100], loss:0.0410
epoch [15/100], loss:0.0372
epoch [16/100], loss:0.0386
epoch [17/100], loss:0.0350
epoch [18/100], loss:0.0356
epoch [19/100], loss:0.0328
epoch [20/100], loss:0.0300
epoch [21/100], loss:0.0320
epoch [22/100], loss:0.0304
epoch [23/100], loss:0.0320
epoch [24/100], loss:0.0325
epoch [25/100], loss:0.0269
epoch [26/100], loss:0.0290
epoch [27/100], loss:0.0287
epoch [28/100], loss:0.0286
epoch [29/100], loss:0.0275
epoch [30/100], loss:0.0283
epoch [31/100], loss:0.0290
epoch [32/100], loss:0.0270
epoch [33/100], loss:0.0236
epoch [34/100], loss:0.0322
epoch [35/100], loss:0.0285
epoch [36/100], loss:0.0268
epoch [37/100], loss:0.0303
epoch [38/100], loss:0.0251
epoch [39/100], loss:0.0271
epoch [40/100], loss:0.0286
epoch [41/100], loss:0.0272
epoch [42/100], loss:0.0286
epoch [43/100], loss:0.0244
epoch [44/100], loss:0.0269
epoch [45/100], loss:0.0238
epoch [46/100], loss:0.0239
epoch [47/100], loss:0.0238
epoch [48/100], loss:0.0251
epoch [49/100], loss:0.0260
epoch [50/100], loss:0.0252
epoch [51/100], loss:0.0254
epoch [52/100], loss:0.0234
epoch [53/100], loss:0.0238
epoch [54/100], loss:0.0238
epoch [55/100], loss:0.0235
epoch [56/100], loss:0.0220
epoch [57/100], loss:0.0252
epoch [58/100], loss:0.0247
epoch [59/100], loss:0.0247
epoch [60/100], loss:0.0238
epoch [61/100], loss:0.0258
epoch [62/100], loss:0.0217
epoch [63/100], loss:0.0218
epoch [64/100], loss:0.0240
epoch [65/100], loss:0.0224
epoch [66/100], loss:0.0217
epoch [67/100], loss:0.0217
epoch [68/100], loss:0.0245
epoch [69/100], loss:0.0240
epoch [70/100], loss:0.0231
epoch [71/100], loss:0.0241
epoch [72/100], loss:0.0211
epoch [73/100], loss:0.0256
epoch [74/100], loss:0.0203
epoch [75/100], loss:0.0238
epoch [76/100], loss:0.0238
epoch [77/100], loss:0.0226
epoch [78/100], loss:0.0226
epoch [79/100], loss:0.0204
epoch [80/100], loss:0.0215
epoch [81/100], loss:0.0201
epoch [82/100], loss:0.0196
epoch [83/100], loss:0.0216
epoch [84/100], loss:0.0228
epoch [85/100], loss:0.0217
epoch [86/100], loss:0.0201
epoch [87/100], loss:0.0214
epoch [88/100], loss:0.0215
epoch [89/100], loss:0.0187
epoch [90/100], loss:0.0205
epoch [91/100], loss:0.0218
epoch [92/100], loss:0.0191
epoch [93/100], loss:0.0200
epoch [94/100], loss:0.0217
epoch [95/100], loss:0.0212
epoch [96/100], loss:0.0218
epoch [97/100], loss:0.0192
epoch [98/100], loss:0.0191
epoch [99/100], loss:0.0184
epoch [100/100], loss:0.0188
Running pruning iteration 3
************pruning 59.04 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 10 / 1152  0.008680555555555556
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 93881 / 131072  0.7162551879882812
encoder.2.bias : 21 / 64  0.328125
encoder.4.weight : 3382 / 8192  0.412841796875
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 1131 / 4608  0.24544270833333334
encoder.6.bias : 2 / 16  0.125
encoder.9.weight : 213 / 1152  0.18489583333333334
encoder.9.bias : 3 / 8  0.375
decoder.0.weight : 4948 / 9216  0.5368923611111112
decoder.0.bias : 4 / 128  0.03125
decoder.2.weight : 13161 / 32768  0.401641845703125
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 3066 / 8192  0.374267578125
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 3976 / 12800  0.310625
decoder.6.bias : 1 / 16  0.0625
decoder.8.weight : 11 / 64  0.171875
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 123810/209705 = 0.5904008011253904
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.4205
epoch [2/100], loss:0.3802
epoch [3/100], loss:0.3954
epoch [4/100], loss:0.4114
epoch [5/100], loss:0.3963
epoch [6/100], loss:0.2174
epoch [7/100], loss:0.0789
epoch [8/100], loss:0.0578
epoch [9/100], loss:0.0461
epoch [10/100], loss:0.0507
epoch [11/100], loss:0.0442
epoch [12/100], loss:0.0431
epoch [13/100], loss:0.0396
epoch [14/100], loss:0.0351
epoch [15/100], loss:0.0393
epoch [16/100], loss:0.0373
epoch [17/100], loss:0.0342
epoch [18/100], loss:0.0317
epoch [19/100], loss:0.0353
epoch [20/100], loss:0.0334
epoch [21/100], loss:0.0327
epoch [22/100], loss:0.0319
epoch [23/100], loss:0.0341
epoch [24/100], loss:0.0312
epoch [25/100], loss:0.0281
epoch [26/100], loss:0.0304
epoch [27/100], loss:0.0277
epoch [28/100], loss:0.0274
epoch [29/100], loss:0.0242
epoch [30/100], loss:0.0278
epoch [31/100], loss:0.0295
epoch [32/100], loss:0.0260
epoch [33/100], loss:0.0257
epoch [34/100], loss:0.0261
epoch [35/100], loss:0.0240
epoch [36/100], loss:0.0254
epoch [37/100], loss:0.0259
epoch [38/100], loss:0.0227
epoch [39/100], loss:0.0243
epoch [40/100], loss:0.0255
epoch [41/100], loss:0.0240
epoch [42/100], loss:0.0275
epoch [43/100], loss:0.0271
epoch [44/100], loss:0.0250
epoch [45/100], loss:0.0243
epoch [46/100], loss:0.0245
epoch [47/100], loss:0.0231
epoch [48/100], loss:0.0219
epoch [49/100], loss:0.0252
epoch [50/100], loss:0.0194
epoch [51/100], loss:0.0204
epoch [52/100], loss:0.0215
epoch [53/100], loss:0.0263
epoch [54/100], loss:0.0226
epoch [55/100], loss:0.0210
epoch [56/100], loss:0.0221
epoch [57/100], loss:0.0258
epoch [58/100], loss:0.0225
epoch [59/100], loss:0.0185
epoch [60/100], loss:0.0255
epoch [61/100], loss:0.0228
epoch [62/100], loss:0.0210
epoch [63/100], loss:0.0226
epoch [64/100], loss:0.0208
epoch [65/100], loss:0.0218
epoch [66/100], loss:0.0235
epoch [67/100], loss:0.0207
epoch [68/100], loss:0.0204
epoch [69/100], loss:0.0184
epoch [70/100], loss:0.0204
epoch [71/100], loss:0.0179
epoch [72/100], loss:0.0216
epoch [73/100], loss:0.0202
epoch [74/100], loss:0.0199
epoch [75/100], loss:0.0231
epoch [76/100], loss:0.0210
epoch [77/100], loss:0.0207
epoch [78/100], loss:0.0218
epoch [79/100], loss:0.0206
epoch [80/100], loss:0.0215
epoch [81/100], loss:0.0200
epoch [82/100], loss:0.0188
epoch [83/100], loss:0.0196
epoch [84/100], loss:0.0196
epoch [85/100], loss:0.0205
epoch [86/100], loss:0.0229
epoch [87/100], loss:0.0226
epoch [88/100], loss:0.0200
epoch [89/100], loss:0.0191
epoch [90/100], loss:0.0199
epoch [91/100], loss:0.0227
epoch [92/100], loss:0.0198
epoch [93/100], loss:0.0214
epoch [94/100], loss:0.0195
epoch [95/100], loss:0.0212
epoch [96/100], loss:0.0190
epoch [97/100], loss:0.0208
epoch [98/100], loss:0.0198
epoch [99/100], loss:0.0193
epoch [100/100], loss:0.0204
Running pruning iteration 4
************pruning 67.232 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 12 / 1152  0.010416666666666666
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 104853 / 131072  0.7999649047851562
encoder.2.bias : 28 / 64  0.4375
encoder.4.weight : 4332 / 8192  0.52880859375
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 1555 / 4608  0.3374565972222222
encoder.6.bias : 4 / 16  0.25
encoder.9.weight : 235 / 1152  0.20399305555555555
encoder.9.bias : 3 / 8  0.375
decoder.0.weight : 5525 / 9216  0.5995008680555556
decoder.0.bias : 4 / 128  0.03125
decoder.2.weight : 14596 / 32768  0.4454345703125
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 3933 / 8192  0.4801025390625
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 5890 / 12800  0.46015625
decoder.6.bias : 2 / 16  0.125
decoder.8.weight : 17 / 64  0.265625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 140989/209705 = 0.6723206409003123
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.4030
epoch [2/100], loss:0.3814
epoch [3/100], loss:0.4147
epoch [4/100], loss:0.1671
epoch [5/100], loss:0.0731
epoch [6/100], loss:0.0722
epoch [7/100], loss:0.0638
epoch [8/100], loss:0.0585
epoch [9/100], loss:0.0521
epoch [10/100], loss:0.0552
epoch [11/100], loss:0.0522
epoch [12/100], loss:0.0535
epoch [13/100], loss:0.0459
epoch [14/100], loss:0.0446
epoch [15/100], loss:0.0397
epoch [16/100], loss:0.0414
epoch [17/100], loss:0.0453
epoch [18/100], loss:0.0451
epoch [19/100], loss:0.0409
epoch [20/100], loss:0.0406
epoch [21/100], loss:0.0377
epoch [22/100], loss:0.0422
epoch [23/100], loss:0.0416
epoch [24/100], loss:0.0414
epoch [25/100], loss:0.0404
epoch [26/100], loss:0.0354
epoch [27/100], loss:0.0366
epoch [28/100], loss:0.0335
epoch [29/100], loss:0.0376
epoch [30/100], loss:0.0346
epoch [31/100], loss:0.0354
epoch [32/100], loss:0.0349
epoch [33/100], loss:0.0354
epoch [34/100], loss:0.0381
epoch [35/100], loss:0.0346
epoch [36/100], loss:0.0339
epoch [37/100], loss:0.0337
epoch [38/100], loss:0.0352
epoch [39/100], loss:0.0307
epoch [40/100], loss:0.0300
epoch [41/100], loss:0.0341
epoch [42/100], loss:0.0361
epoch [43/100], loss:0.0345
epoch [44/100], loss:0.0306
epoch [45/100], loss:0.0323
epoch [46/100], loss:0.0326
epoch [47/100], loss:0.0326
epoch [48/100], loss:0.0321
epoch [49/100], loss:0.0310
epoch [50/100], loss:0.0346
epoch [51/100], loss:0.0308
epoch [52/100], loss:0.0311
epoch [53/100], loss:0.0315
epoch [54/100], loss:0.0320
epoch [55/100], loss:0.0319
epoch [56/100], loss:0.0339
epoch [57/100], loss:0.0343
epoch [58/100], loss:0.0308
epoch [59/100], loss:0.0304
epoch [60/100], loss:0.0352
epoch [61/100], loss:0.0279
epoch [62/100], loss:0.0293
epoch [63/100], loss:0.0287
epoch [64/100], loss:0.0285
epoch [65/100], loss:0.0317
epoch [66/100], loss:0.0289
epoch [67/100], loss:0.0309
epoch [68/100], loss:0.0293
epoch [69/100], loss:0.0291
epoch [70/100], loss:0.0299
epoch [71/100], loss:0.0262
epoch [72/100], loss:0.0298
epoch [73/100], loss:0.0286
epoch [74/100], loss:0.0303
epoch [75/100], loss:0.0273
epoch [76/100], loss:0.0287
epoch [77/100], loss:0.0335
epoch [78/100], loss:0.0268
epoch [79/100], loss:0.0308
epoch [80/100], loss:0.0262
epoch [81/100], loss:0.0284
epoch [82/100], loss:0.0291
epoch [83/100], loss:0.0266
epoch [84/100], loss:0.0271
epoch [85/100], loss:0.0309
epoch [86/100], loss:0.0265
epoch [87/100], loss:0.0285
epoch [88/100], loss:0.0242
epoch [89/100], loss:0.0271
epoch [90/100], loss:0.0241
epoch [91/100], loss:0.0274
epoch [92/100], loss:0.0272
epoch [93/100], loss:0.0242
epoch [94/100], loss:0.0261
epoch [95/100], loss:0.0292
epoch [96/100], loss:0.0293
epoch [97/100], loss:0.0299
epoch [98/100], loss:0.0283
epoch [99/100], loss:0.0269
epoch [100/100], loss:0.0255
Running pruning iteration 5
************pruning 73.7856 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 89 / 1152  0.07725694444444445
encoder.0.bias : 8 / 128  0.0625
encoder.2.weight : 111119 / 131072  0.8477706909179688
encoder.2.bias : 34 / 64  0.53125
encoder.4.weight : 5272 / 8192  0.6435546875
encoder.4.bias : 2 / 32  0.0625
encoder.6.weight : 1798 / 4608  0.3901909722222222
encoder.6.bias : 5 / 16  0.3125
encoder.9.weight : 370 / 1152  0.3211805555555556
encoder.9.bias : 4 / 8  0.5
decoder.0.weight : 6619 / 9216  0.7182074652777778
decoder.0.bias : 4 / 128  0.03125
decoder.2.weight : 18041 / 32768  0.550567626953125
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 4726 / 8192  0.576904296875
decoder.4.bias : 2 / 32  0.0625
decoder.6.weight : 6619 / 12800  0.517109375
decoder.6.bias : 3 / 16  0.1875
decoder.8.weight : 17 / 64  0.265625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 154732/209705 = 0.737855558999547
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.3999
epoch [2/100], loss:0.3934
epoch [3/100], loss:0.3815
epoch [4/100], loss:0.4063
epoch [5/100], loss:0.2050
epoch [6/100], loss:0.0811
epoch [7/100], loss:0.0594
epoch [8/100], loss:0.0575
epoch [9/100], loss:0.0506
epoch [10/100], loss:0.0475
epoch [11/100], loss:0.0456
epoch [12/100], loss:0.0404
epoch [13/100], loss:0.0411
epoch [14/100], loss:0.0399
epoch [15/100], loss:0.0376
epoch [16/100], loss:0.0357
epoch [17/100], loss:0.0347
epoch [18/100], loss:0.0357
epoch [19/100], loss:0.0350
epoch [20/100], loss:0.0361
epoch [21/100], loss:0.0351
epoch [22/100], loss:0.0369
epoch [23/100], loss:0.0314
epoch [24/100], loss:0.0354
epoch [25/100], loss:0.0343
epoch [26/100], loss:0.0377
epoch [27/100], loss:0.0345
epoch [28/100], loss:0.0326
epoch [29/100], loss:0.0344
epoch [30/100], loss:0.0340
epoch [31/100], loss:0.0323
epoch [32/100], loss:0.0294
epoch [33/100], loss:0.0312
epoch [34/100], loss:0.0284
epoch [35/100], loss:0.0308
epoch [36/100], loss:0.0306
epoch [37/100], loss:0.0293
epoch [38/100], loss:0.0293
epoch [39/100], loss:0.0321
epoch [40/100], loss:0.0278
epoch [41/100], loss:0.0330
epoch [42/100], loss:0.0286
epoch [43/100], loss:0.0306
epoch [44/100], loss:0.0289
epoch [45/100], loss:0.0306
epoch [46/100], loss:0.0293
epoch [47/100], loss:0.0290
epoch [48/100], loss:0.0264
epoch [49/100], loss:0.0300
epoch [50/100], loss:0.0280
epoch [51/100], loss:0.0272
epoch [52/100], loss:0.0283
epoch [53/100], loss:0.0252
epoch [54/100], loss:0.0260
epoch [55/100], loss:0.0240
epoch [56/100], loss:0.0248
epoch [57/100], loss:0.0259
epoch [58/100], loss:0.0281
epoch [59/100], loss:0.0276
epoch [60/100], loss:0.0256
epoch [61/100], loss:0.0284
epoch [62/100], loss:0.0293
epoch [63/100], loss:0.0223
epoch [64/100], loss:0.0245
epoch [65/100], loss:0.0258
epoch [66/100], loss:0.0255
epoch [67/100], loss:0.0233
epoch [68/100], loss:0.0269
epoch [69/100], loss:0.0241
epoch [70/100], loss:0.0258
epoch [71/100], loss:0.0250
epoch [72/100], loss:0.0229
epoch [73/100], loss:0.0256
epoch [74/100], loss:0.0255
epoch [75/100], loss:0.0264
epoch [76/100], loss:0.0244
epoch [77/100], loss:0.0248
epoch [78/100], loss:0.0220
epoch [79/100], loss:0.0206
epoch [80/100], loss:0.0223
epoch [81/100], loss:0.0258
epoch [82/100], loss:0.0216
epoch [83/100], loss:0.0227
epoch [84/100], loss:0.0242
epoch [85/100], loss:0.0213
epoch [86/100], loss:0.0255
epoch [87/100], loss:0.0219
epoch [88/100], loss:0.0234
epoch [89/100], loss:0.0245
epoch [90/100], loss:0.0235
epoch [91/100], loss:0.0234
epoch [92/100], loss:0.0236
epoch [93/100], loss:0.0233
epoch [94/100], loss:0.0219
epoch [95/100], loss:0.0226
epoch [96/100], loss:0.0202
epoch [97/100], loss:0.0238
epoch [98/100], loss:0.0225
epoch [99/100], loss:0.0222
epoch [100/100], loss:0.0250
Running pruning iteration 6
************pruning 79.02848 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 290 / 1152  0.2517361111111111
encoder.0.bias : 30 / 128  0.234375
encoder.2.weight : 116698 / 131072  0.8903350830078125
encoder.2.bias : 43 / 64  0.671875
encoder.4.weight : 6678 / 8192  0.815185546875
encoder.4.bias : 5 / 32  0.15625
encoder.6.weight : 2273 / 4608  0.4932725694444444
encoder.6.bias : 5 / 16  0.3125
encoder.9.weight : 406 / 1152  0.3524305555555556
encoder.9.bias : 4 / 8  0.5
decoder.0.weight : 7011 / 9216  0.7607421875
decoder.0.bias : 11 / 128  0.0859375
decoder.2.weight : 20218 / 32768  0.61700439453125
decoder.2.bias : 5 / 64  0.078125
decoder.4.weight : 5345 / 8192  0.6524658203125
decoder.4.bias : 6 / 32  0.1875
decoder.6.weight : 6678 / 12800  0.52171875
decoder.6.bias : 3 / 16  0.1875
decoder.8.weight : 17 / 64  0.265625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 165726/209705 = 0.790281586037529
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.4072
epoch [2/100], loss:0.4186
epoch [3/100], loss:0.3999
epoch [4/100], loss:0.4076
epoch [5/100], loss:0.1491
epoch [6/100], loss:0.0840
epoch [7/100], loss:0.0601
epoch [8/100], loss:0.0567
epoch [9/100], loss:0.0522
epoch [10/100], loss:0.0502
epoch [11/100], loss:0.0449
epoch [12/100], loss:0.0473
epoch [13/100], loss:0.0431
epoch [14/100], loss:0.0437
epoch [15/100], loss:0.0468
epoch [16/100], loss:0.0360
epoch [17/100], loss:0.0375
epoch [18/100], loss:0.0426
epoch [19/100], loss:0.0375
epoch [20/100], loss:0.0410
epoch [21/100], loss:0.0360
epoch [22/100], loss:0.0366
epoch [23/100], loss:0.0392
epoch [24/100], loss:0.0324
epoch [25/100], loss:0.0413
epoch [26/100], loss:0.0357
epoch [27/100], loss:0.0357
epoch [28/100], loss:0.0408
epoch [29/100], loss:0.0337
epoch [30/100], loss:0.0369
epoch [31/100], loss:0.0322
epoch [32/100], loss:0.0309
epoch [33/100], loss:0.0301
epoch [34/100], loss:0.0365
epoch [35/100], loss:0.0297
epoch [36/100], loss:0.0327
epoch [37/100], loss:0.0314
epoch [38/100], loss:0.0342
epoch [39/100], loss:0.0290
epoch [40/100], loss:0.0310
epoch [41/100], loss:0.0312
epoch [42/100], loss:0.0342
epoch [43/100], loss:0.0325
epoch [44/100], loss:0.0320
epoch [45/100], loss:0.0287
epoch [46/100], loss:0.0335
epoch [47/100], loss:0.0300
epoch [48/100], loss:0.0322
epoch [49/100], loss:0.0271
epoch [50/100], loss:0.0299
epoch [51/100], loss:0.0257
epoch [52/100], loss:0.0337
epoch [53/100], loss:0.0258
epoch [54/100], loss:0.0286
epoch [55/100], loss:0.0274
epoch [56/100], loss:0.0279
epoch [57/100], loss:0.0280
epoch [58/100], loss:0.0258
epoch [59/100], loss:0.0266
epoch [60/100], loss:0.0281
epoch [61/100], loss:0.0270
epoch [62/100], loss:0.0283
epoch [63/100], loss:0.0264
epoch [64/100], loss:0.0255
epoch [65/100], loss:0.0292
epoch [66/100], loss:0.0276
epoch [67/100], loss:0.0279
epoch [68/100], loss:0.0304
epoch [69/100], loss:0.0292
epoch [70/100], loss:0.0248
epoch [71/100], loss:0.0221
epoch [72/100], loss:0.0282
epoch [73/100], loss:0.0288
epoch [74/100], loss:0.0254
epoch [75/100], loss:0.0252
epoch [76/100], loss:0.0273
epoch [77/100], loss:0.0267
epoch [78/100], loss:0.0279
epoch [79/100], loss:0.0283
epoch [80/100], loss:0.0252
epoch [81/100], loss:0.0244
epoch [82/100], loss:0.0263
epoch [83/100], loss:0.0260
epoch [84/100], loss:0.0252
epoch [85/100], loss:0.0260
epoch [86/100], loss:0.0264
epoch [87/100], loss:0.0255
epoch [88/100], loss:0.0268
epoch [89/100], loss:0.0265
epoch [90/100], loss:0.0263
epoch [91/100], loss:0.0226
epoch [92/100], loss:0.0260
epoch [93/100], loss:0.0254
epoch [94/100], loss:0.0250
epoch [95/100], loss:0.0224
epoch [96/100], loss:0.0258
epoch [97/100], loss:0.0271
epoch [98/100], loss:0.0238
epoch [99/100], loss:0.0253
epoch [100/100], loss:0.0265
Running pruning iteration 7
************pruning 83.222784 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 359 / 1152  0.3116319444444444
encoder.0.bias : 41 / 128  0.3203125
encoder.2.weight : 120449 / 131072  0.9189529418945312
encoder.2.bias : 48 / 64  0.75
encoder.4.weight : 7095 / 8192  0.8660888671875
encoder.4.bias : 15 / 32  0.46875
encoder.6.weight : 2671 / 4608  0.5796440972222222
encoder.6.bias : 7 / 16  0.4375
encoder.9.weight : 560 / 1152  0.4861111111111111
encoder.9.bias : 6 / 8  0.75
decoder.0.weight : 7519 / 9216  0.8158637152777778
decoder.0.bias : 30 / 128  0.234375
decoder.2.weight : 22936 / 32768  0.699951171875
decoder.2.bias : 11 / 64  0.171875
decoder.4.weight : 5541 / 8192  0.6763916015625
decoder.4.bias : 10 / 32  0.3125
decoder.6.weight : 7202 / 12800  0.56265625
decoder.6.bias : 4 / 16  0.25
decoder.8.weight : 18 / 64  0.28125
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 174522/209705 = 0.832226222550726
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.4390
epoch [2/100], loss:0.3828
epoch [3/100], loss:0.3979
epoch [4/100], loss:0.4118
epoch [5/100], loss:0.2216
epoch [6/100], loss:0.0906
epoch [7/100], loss:0.0809
epoch [8/100], loss:0.0663
epoch [9/100], loss:0.0665
epoch [10/100], loss:0.0609
epoch [11/100], loss:0.0598
epoch [12/100], loss:0.0623
epoch [13/100], loss:0.0544
epoch [14/100], loss:0.0577
epoch [15/100], loss:0.0496
epoch [16/100], loss:0.0503
epoch [17/100], loss:0.0487
epoch [18/100], loss:0.0466
epoch [19/100], loss:0.0441
epoch [20/100], loss:0.0449
epoch [21/100], loss:0.0443
epoch [22/100], loss:0.0441
epoch [23/100], loss:0.0379
epoch [24/100], loss:0.0436
epoch [25/100], loss:0.0389
epoch [26/100], loss:0.0371
epoch [27/100], loss:0.0391
epoch [28/100], loss:0.0365
epoch [29/100], loss:0.0399
epoch [30/100], loss:0.0391
epoch [31/100], loss:0.0345
epoch [32/100], loss:0.0356
epoch [33/100], loss:0.0414
epoch [34/100], loss:0.0372
epoch [35/100], loss:0.0342
epoch [36/100], loss:0.0383
epoch [37/100], loss:0.0378
epoch [38/100], loss:0.0353
epoch [39/100], loss:0.0379
epoch [40/100], loss:0.0310
epoch [41/100], loss:0.0348
epoch [42/100], loss:0.0359
epoch [43/100], loss:0.0343
epoch [44/100], loss:0.0324
epoch [45/100], loss:0.0333
epoch [46/100], loss:0.0350
epoch [47/100], loss:0.0330
epoch [48/100], loss:0.0314
epoch [49/100], loss:0.0329
epoch [50/100], loss:0.0319
epoch [51/100], loss:0.0320
epoch [52/100], loss:0.0337
epoch [53/100], loss:0.0324
epoch [54/100], loss:0.0313
epoch [55/100], loss:0.0351
epoch [56/100], loss:0.0297
epoch [57/100], loss:0.0363
epoch [58/100], loss:0.0325
epoch [59/100], loss:0.0342
epoch [60/100], loss:0.0310
epoch [61/100], loss:0.0290
epoch [62/100], loss:0.0347
epoch [63/100], loss:0.0275
epoch [64/100], loss:0.0272
epoch [65/100], loss:0.0347
epoch [66/100], loss:0.0301
epoch [67/100], loss:0.0250
epoch [68/100], loss:0.0310
epoch [69/100], loss:0.0268
epoch [70/100], loss:0.0310
epoch [71/100], loss:0.0280
epoch [72/100], loss:0.0275
epoch [73/100], loss:0.0305
epoch [74/100], loss:0.0293
epoch [75/100], loss:0.0302
epoch [76/100], loss:0.0297
epoch [77/100], loss:0.0259
epoch [78/100], loss:0.0327
epoch [79/100], loss:0.0265
epoch [80/100], loss:0.0267
epoch [81/100], loss:0.0293
epoch [82/100], loss:0.0261
epoch [83/100], loss:0.0262
epoch [84/100], loss:0.0303
epoch [85/100], loss:0.0296
epoch [86/100], loss:0.0261
epoch [87/100], loss:0.0297
epoch [88/100], loss:0.0280
epoch [89/100], loss:0.0305
epoch [90/100], loss:0.0274
epoch [91/100], loss:0.0256
epoch [92/100], loss:0.0313
epoch [93/100], loss:0.0246
epoch [94/100], loss:0.0259
epoch [95/100], loss:0.0264
epoch [96/100], loss:0.0252
epoch [97/100], loss:0.0287
epoch [98/100], loss:0.0264
epoch [99/100], loss:0.0256
epoch [100/100], loss:0.0268
Running pruning iteration 8
************pruning 86.5782272 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 410 / 1152  0.3559027777777778
encoder.0.bias : 44 / 128  0.34375
encoder.2.weight : 122426 / 131072  0.9340362548828125
encoder.2.bias : 51 / 64  0.796875
encoder.4.weight : 7159 / 8192  0.8739013671875
encoder.4.bias : 25 / 32  0.78125
encoder.6.weight : 2861 / 4608  0.6208767361111112
encoder.6.bias : 8 / 16  0.5
encoder.9.weight : 648 / 1152  0.5625
encoder.9.bias : 6 / 8  0.75
decoder.0.weight : 7990 / 9216  0.8669704861111112
decoder.0.bias : 47 / 128  0.3671875
decoder.2.weight : 26118 / 32768  0.79705810546875
decoder.2.bias : 21 / 64  0.328125
decoder.4.weight : 5974 / 8192  0.729248046875
decoder.4.bias : 13 / 32  0.40625
decoder.6.weight : 7735 / 12800  0.604296875
decoder.6.bias : 5 / 16  0.3125
decoder.8.weight : 18 / 64  0.28125
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 181559/209705 = 0.8657828854819866
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.4184
epoch [2/100], loss:0.4421
epoch [3/100], loss:0.4096
epoch [4/100], loss:0.2097
epoch [5/100], loss:0.0929
epoch [6/100], loss:0.0783
epoch [7/100], loss:0.0712
epoch [8/100], loss:0.0716
epoch [9/100], loss:0.0630
epoch [10/100], loss:0.0620
epoch [11/100], loss:0.0609
epoch [12/100], loss:0.0678
epoch [13/100], loss:0.0513
epoch [14/100], loss:0.0559
epoch [15/100], loss:0.0565
epoch [16/100], loss:0.0478
epoch [17/100], loss:0.0533
epoch [18/100], loss:0.0516
epoch [19/100], loss:0.0505
epoch [20/100], loss:0.0505
epoch [21/100], loss:0.0487
epoch [22/100], loss:0.0524
epoch [23/100], loss:0.0484
epoch [24/100], loss:0.0515
epoch [25/100], loss:0.0460
epoch [26/100], loss:0.0478
epoch [27/100], loss:0.0428
epoch [28/100], loss:0.0451
epoch [29/100], loss:0.0433
epoch [30/100], loss:0.0430
epoch [31/100], loss:0.0522
epoch [32/100], loss:0.0440
epoch [33/100], loss:0.0506
epoch [34/100], loss:0.0421
epoch [35/100], loss:0.0487
epoch [36/100], loss:0.0463
epoch [37/100], loss:0.0394
epoch [38/100], loss:0.0424
epoch [39/100], loss:0.0444
epoch [40/100], loss:0.0433
epoch [41/100], loss:0.0427
epoch [42/100], loss:0.0416
epoch [43/100], loss:0.0404
epoch [44/100], loss:0.0438
epoch [45/100], loss:0.0414
epoch [46/100], loss:0.0404
epoch [47/100], loss:0.0391
epoch [48/100], loss:0.0390
epoch [49/100], loss:0.0424
epoch [50/100], loss:0.0413
epoch [51/100], loss:0.0424
epoch [52/100], loss:0.0406
epoch [53/100], loss:0.0384
epoch [54/100], loss:0.0406
epoch [55/100], loss:0.0381
epoch [56/100], loss:0.0415
epoch [57/100], loss:0.0397
epoch [58/100], loss:0.0369
epoch [59/100], loss:0.0375
epoch [60/100], loss:0.0349
epoch [61/100], loss:0.0367
epoch [62/100], loss:0.0330
epoch [63/100], loss:0.0362
epoch [64/100], loss:0.0320
epoch [65/100], loss:0.0378
epoch [66/100], loss:0.0365
epoch [67/100], loss:0.0378
epoch [68/100], loss:0.0338
epoch [69/100], loss:0.0390
epoch [70/100], loss:0.0370
epoch [71/100], loss:0.0359
epoch [72/100], loss:0.0378
epoch [73/100], loss:0.0395
epoch [74/100], loss:0.0398
epoch [75/100], loss:0.0348
epoch [76/100], loss:0.0385
epoch [77/100], loss:0.0331
epoch [78/100], loss:0.0357
epoch [79/100], loss:0.0374
epoch [80/100], loss:0.0350
epoch [81/100], loss:0.0362
epoch [82/100], loss:0.0365
epoch [83/100], loss:0.0342
epoch [84/100], loss:0.0337
epoch [85/100], loss:0.0345
epoch [86/100], loss:0.0361
epoch [87/100], loss:0.0374
epoch [88/100], loss:0.0340
epoch [89/100], loss:0.0349
epoch [90/100], loss:0.0371
epoch [91/100], loss:0.0354
epoch [92/100], loss:0.0334
epoch [93/100], loss:0.0380
epoch [94/100], loss:0.0325
epoch [95/100], loss:0.0307
epoch [96/100], loss:0.0349
epoch [97/100], loss:0.0343
epoch [98/100], loss:0.0320
epoch [99/100], loss:0.0388
epoch [100/100], loss:0.0293
Running pruning iteration 9
************pruning 89.26258176 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 454 / 1152  0.3940972222222222
encoder.0.bias : 51 / 128  0.3984375
encoder.2.weight : 124225 / 131072  0.9477615356445312
encoder.2.bias : 56 / 64  0.875
encoder.4.weight : 7253 / 8192  0.8853759765625
encoder.4.bias : 28 / 32  0.875
encoder.6.weight : 3349 / 4608  0.7267795138888888
encoder.6.bias : 10 / 16  0.625
encoder.9.weight : 839 / 1152  0.7282986111111112
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8315 / 9216  0.9022352430555556
decoder.0.bias : 58 / 128  0.453125
decoder.2.weight : 27752 / 32768  0.846923828125
decoder.2.bias : 25 / 64  0.390625
decoder.4.weight : 6340 / 8192  0.77392578125
decoder.4.bias : 16 / 32  0.5
decoder.6.weight : 8385 / 12800  0.655078125
decoder.6.bias : 7 / 16  0.4375
decoder.8.weight : 18 / 64  0.28125
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 187188/209705 = 0.8926253546648864
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1874
epoch [2/100], loss:0.1156
epoch [3/100], loss:0.0938
epoch [4/100], loss:0.0713
epoch [5/100], loss:0.0772
epoch [6/100], loss:0.0695
epoch [7/100], loss:0.0652
epoch [8/100], loss:0.0630
epoch [9/100], loss:0.0699
epoch [10/100], loss:0.0635
epoch [11/100], loss:0.0594
epoch [12/100], loss:0.0547
epoch [13/100], loss:0.0539
epoch [14/100], loss:0.0532
epoch [15/100], loss:0.0597
epoch [16/100], loss:0.0513
epoch [17/100], loss:0.0565
epoch [18/100], loss:0.0547
epoch [19/100], loss:0.0564
epoch [20/100], loss:0.0529
epoch [21/100], loss:0.0533
epoch [22/100], loss:0.0490
epoch [23/100], loss:0.0430
epoch [24/100], loss:0.0487
epoch [25/100], loss:0.0522
epoch [26/100], loss:0.0473
epoch [27/100], loss:0.0516
epoch [28/100], loss:0.0469
epoch [29/100], loss:0.0471
epoch [30/100], loss:0.0499
epoch [31/100], loss:0.0500
epoch [32/100], loss:0.0521
epoch [33/100], loss:0.0480
epoch [34/100], loss:0.0457
epoch [35/100], loss:0.0475
epoch [36/100], loss:0.0457
epoch [37/100], loss:0.0472
epoch [38/100], loss:0.0445
epoch [39/100], loss:0.0422
epoch [40/100], loss:0.0433
epoch [41/100], loss:0.0451
epoch [42/100], loss:0.0423
epoch [43/100], loss:0.0429
epoch [44/100], loss:0.0423
epoch [45/100], loss:0.0448
epoch [46/100], loss:0.0469
epoch [47/100], loss:0.0419
epoch [48/100], loss:0.0462
epoch [49/100], loss:0.0423
epoch [50/100], loss:0.0404
epoch [51/100], loss:0.0450
epoch [52/100], loss:0.0412
epoch [53/100], loss:0.0356
epoch [54/100], loss:0.0405
epoch [55/100], loss:0.0428
epoch [56/100], loss:0.0424
epoch [57/100], loss:0.0417
epoch [58/100], loss:0.0392
epoch [59/100], loss:0.0398
epoch [60/100], loss:0.0395
epoch [61/100], loss:0.0396
epoch [62/100], loss:0.0407
epoch [63/100], loss:0.0390
epoch [64/100], loss:0.0371
epoch [65/100], loss:0.0367
epoch [66/100], loss:0.0388
epoch [67/100], loss:0.0368
epoch [68/100], loss:0.0356
epoch [69/100], loss:0.0368
epoch [70/100], loss:0.0378
epoch [71/100], loss:0.0429
epoch [72/100], loss:0.0364
epoch [73/100], loss:0.0347
epoch [74/100], loss:0.0354
epoch [75/100], loss:0.0408
epoch [76/100], loss:0.0397
epoch [77/100], loss:0.0342
epoch [78/100], loss:0.0344
epoch [79/100], loss:0.0374
epoch [80/100], loss:0.0389
epoch [81/100], loss:0.0362
epoch [82/100], loss:0.0383
epoch [83/100], loss:0.0386
epoch [84/100], loss:0.0399
epoch [85/100], loss:0.0360
epoch [86/100], loss:0.0397
epoch [87/100], loss:0.0404
epoch [88/100], loss:0.0373
epoch [89/100], loss:0.0353
epoch [90/100], loss:0.0343
epoch [91/100], loss:0.0336
epoch [92/100], loss:0.0331
epoch [93/100], loss:0.0367
epoch [94/100], loss:0.0369
epoch [95/100], loss:0.0365
epoch [96/100], loss:0.0383
epoch [97/100], loss:0.0340
epoch [98/100], loss:0.0375
epoch [99/100], loss:0.0374
epoch [100/100], loss:0.0338
Running pruning iteration 10
************pruning 91.41006540800001 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 529 / 1152  0.4592013888888889
encoder.0.bias : 55 / 128  0.4296875
encoder.2.weight : 125960 / 131072  0.96099853515625
encoder.2.bias : 56 / 64  0.875
encoder.4.weight : 7350 / 8192  0.897216796875
encoder.4.bias : 30 / 32  0.9375
encoder.6.weight : 3518 / 4608  0.7634548611111112
encoder.6.bias : 13 / 16  0.8125
encoder.9.weight : 905 / 1152  0.7855902777777778
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8549 / 9216  0.9276258680555556
decoder.0.bias : 70 / 128  0.546875
decoder.2.weight : 28775 / 32768  0.878143310546875
decoder.2.bias : 27 / 64  0.421875
decoder.4.weight : 6582 / 8192  0.803466796875
decoder.4.bias : 19 / 32  0.59375
decoder.6.weight : 9220 / 12800  0.7203125
decoder.6.bias : 8 / 16  0.5
decoder.8.weight : 18 / 64  0.28125
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 191691/209705 = 0.9140983762905033
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.2100
epoch [2/100], loss:0.1327
epoch [3/100], loss:0.1136
epoch [4/100], loss:0.0948
epoch [5/100], loss:0.0823
epoch [6/100], loss:0.0886
epoch [7/100], loss:0.0806
epoch [8/100], loss:0.0723
epoch [9/100], loss:0.0727
epoch [10/100], loss:0.0699
epoch [11/100], loss:0.0765
epoch [12/100], loss:0.0610
epoch [13/100], loss:0.0695
epoch [14/100], loss:0.0591
epoch [15/100], loss:0.0657
epoch [16/100], loss:0.0646
epoch [17/100], loss:0.0630
epoch [18/100], loss:0.0577
epoch [19/100], loss:0.0553
epoch [20/100], loss:0.0564
epoch [21/100], loss:0.0597
epoch [22/100], loss:0.0626
epoch [23/100], loss:0.0627
epoch [24/100], loss:0.0535
epoch [25/100], loss:0.0518
epoch [26/100], loss:0.0555
epoch [27/100], loss:0.0531
epoch [28/100], loss:0.0539
epoch [29/100], loss:0.0517
epoch [30/100], loss:0.0503
epoch [31/100], loss:0.0518
epoch [32/100], loss:0.0506
epoch [33/100], loss:0.0479
epoch [34/100], loss:0.0488
epoch [35/100], loss:0.0472
epoch [36/100], loss:0.0448
epoch [37/100], loss:0.0491
epoch [38/100], loss:0.0426
epoch [39/100], loss:0.0433
epoch [40/100], loss:0.0497
epoch [41/100], loss:0.0464
epoch [42/100], loss:0.0441
epoch [43/100], loss:0.0503
epoch [44/100], loss:0.0451
epoch [45/100], loss:0.0474
epoch [46/100], loss:0.0485
epoch [47/100], loss:0.0426
epoch [48/100], loss:0.0483
epoch [49/100], loss:0.0470
epoch [50/100], loss:0.0441
epoch [51/100], loss:0.0457
epoch [52/100], loss:0.0449
epoch [53/100], loss:0.0427
epoch [54/100], loss:0.0415
epoch [55/100], loss:0.0455
epoch [56/100], loss:0.0472
epoch [57/100], loss:0.0422
epoch [58/100], loss:0.0468
epoch [59/100], loss:0.0449
epoch [60/100], loss:0.0433
epoch [61/100], loss:0.0408
epoch [62/100], loss:0.0380
epoch [63/100], loss:0.0405
epoch [64/100], loss:0.0364
epoch [65/100], loss:0.0390
epoch [66/100], loss:0.0365
epoch [67/100], loss:0.0441
epoch [68/100], loss:0.0400
epoch [69/100], loss:0.0450
epoch [70/100], loss:0.0413
epoch [71/100], loss:0.0400
epoch [72/100], loss:0.0407
epoch [73/100], loss:0.0387
epoch [74/100], loss:0.0386
epoch [75/100], loss:0.0399
epoch [76/100], loss:0.0352
epoch [77/100], loss:0.0394
epoch [78/100], loss:0.0408
epoch [79/100], loss:0.0362
epoch [80/100], loss:0.0376
epoch [81/100], loss:0.0388
epoch [82/100], loss:0.0426
epoch [83/100], loss:0.0364
epoch [84/100], loss:0.0392
epoch [85/100], loss:0.0364
epoch [86/100], loss:0.0405
epoch [87/100], loss:0.0405
epoch [88/100], loss:0.0352
epoch [89/100], loss:0.0384
epoch [90/100], loss:0.0360
epoch [91/100], loss:0.0358
epoch [92/100], loss:0.0332
epoch [93/100], loss:0.0413
epoch [94/100], loss:0.0395
epoch [95/100], loss:0.0322
epoch [96/100], loss:0.0409
epoch [97/100], loss:0.0336
epoch [98/100], loss:0.0355
epoch [99/100], loss:0.0364
epoch [100/100], loss:0.0351
Running pruning iteration 11
************pruning 93.12805232640001 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 595 / 1152  0.5164930555555556
encoder.0.bias : 58 / 128  0.453125
encoder.2.weight : 127095 / 131072  0.9696578979492188
encoder.2.bias : 58 / 64  0.90625
encoder.4.weight : 7476 / 8192  0.91259765625
encoder.4.bias : 31 / 32  0.96875
encoder.6.weight : 3775 / 4608  0.8192274305555556
encoder.6.bias : 13 / 16  0.8125
encoder.9.weight : 976 / 1152  0.8472222222222222
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8738 / 9216  0.9481336805555556
decoder.0.bias : 76 / 128  0.59375
decoder.2.weight : 29754 / 32768  0.90802001953125
decoder.2.bias : 29 / 64  0.453125
decoder.4.weight : 6753 / 8192  0.8243408203125
decoder.4.bias : 19 / 32  0.59375
decoder.6.weight : 9814 / 12800  0.76671875
decoder.6.bias : 8 / 16  0.5
decoder.8.weight : 18 / 64  0.28125
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 195294/209705 = 0.9312796547531056
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1550
epoch [2/100], loss:0.1239
epoch [3/100], loss:0.1096
epoch [4/100], loss:0.1009
epoch [5/100], loss:0.0896
epoch [6/100], loss:0.0871
epoch [7/100], loss:0.0848
epoch [8/100], loss:0.0773
epoch [9/100], loss:0.0850
epoch [10/100], loss:0.0886
epoch [11/100], loss:0.0749
epoch [12/100], loss:0.0658
epoch [13/100], loss:0.0751
epoch [14/100], loss:0.0716
epoch [15/100], loss:0.0719
epoch [16/100], loss:0.0752
epoch [17/100], loss:0.0670
epoch [18/100], loss:0.0689
epoch [19/100], loss:0.0658
epoch [20/100], loss:0.0623
epoch [21/100], loss:0.0608
epoch [22/100], loss:0.0600
epoch [23/100], loss:0.0671
epoch [24/100], loss:0.0622
epoch [25/100], loss:0.0650
epoch [26/100], loss:0.0562
epoch [27/100], loss:0.0604
epoch [28/100], loss:0.0543
epoch [29/100], loss:0.0675
epoch [30/100], loss:0.0555
epoch [31/100], loss:0.0632
epoch [32/100], loss:0.0600
epoch [33/100], loss:0.0558
epoch [34/100], loss:0.0572
epoch [35/100], loss:0.0585
epoch [36/100], loss:0.0595
epoch [37/100], loss:0.0550
epoch [38/100], loss:0.0582
epoch [39/100], loss:0.0512
epoch [40/100], loss:0.0630
epoch [41/100], loss:0.0541
epoch [42/100], loss:0.0527
epoch [43/100], loss:0.0521
epoch [44/100], loss:0.0495
epoch [45/100], loss:0.0529
epoch [46/100], loss:0.0477
epoch [47/100], loss:0.0489
epoch [48/100], loss:0.0500
epoch [49/100], loss:0.0492
epoch [50/100], loss:0.0540
epoch [51/100], loss:0.0519
epoch [52/100], loss:0.0550
epoch [53/100], loss:0.0545
epoch [54/100], loss:0.0493
epoch [55/100], loss:0.0485
epoch [56/100], loss:0.0520
epoch [57/100], loss:0.0557
epoch [58/100], loss:0.0482
epoch [59/100], loss:0.0482
epoch [60/100], loss:0.0488
epoch [61/100], loss:0.0475
epoch [62/100], loss:0.0501
epoch [63/100], loss:0.0486
epoch [64/100], loss:0.0459
epoch [65/100], loss:0.0477
epoch [66/100], loss:0.0474
epoch [67/100], loss:0.0522
epoch [68/100], loss:0.0484
epoch [69/100], loss:0.0535
epoch [70/100], loss:0.0500
epoch [71/100], loss:0.0500
epoch [72/100], loss:0.0480
epoch [73/100], loss:0.0464
epoch [74/100], loss:0.0495
epoch [75/100], loss:0.0498
epoch [76/100], loss:0.0474
epoch [77/100], loss:0.0475
epoch [78/100], loss:0.0445
epoch [79/100], loss:0.0461
epoch [80/100], loss:0.0472
epoch [81/100], loss:0.0475
epoch [82/100], loss:0.0410
epoch [83/100], loss:0.0409
epoch [84/100], loss:0.0447
epoch [85/100], loss:0.0475
epoch [86/100], loss:0.0453
epoch [87/100], loss:0.0417
epoch [88/100], loss:0.0468
epoch [89/100], loss:0.0423
epoch [90/100], loss:0.0458
epoch [91/100], loss:0.0459
epoch [92/100], loss:0.0457
epoch [93/100], loss:0.0465
epoch [94/100], loss:0.0455
epoch [95/100], loss:0.0430
epoch [96/100], loss:0.0426
epoch [97/100], loss:0.0480
epoch [98/100], loss:0.0435
epoch [99/100], loss:0.0436
epoch [100/100], loss:0.0441
Running pruning iteration 12
************pruning 94.50244186112 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 629 / 1152  0.5460069444444444
encoder.0.bias : 61 / 128  0.4765625
encoder.2.weight : 127760 / 131072  0.9747314453125
encoder.2.bias : 58 / 64  0.90625
encoder.4.weight : 7537 / 8192  0.9200439453125
encoder.4.bias : 31 / 32  0.96875
encoder.6.weight : 3871 / 4608  0.8400607638888888
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 1011 / 1152  0.8776041666666666
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8844 / 9216  0.9596354166666666
decoder.0.bias : 84 / 128  0.65625
decoder.2.weight : 30436 / 32768  0.9288330078125
decoder.2.bias : 29 / 64  0.453125
decoder.4.weight : 6936 / 8192  0.8466796875
decoder.4.bias : 22 / 32  0.6875
decoder.6.weight : 10807 / 12800  0.844296875
decoder.6.bias : 11 / 16  0.6875
decoder.8.weight : 26 / 64  0.40625
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 198176/209705 = 0.9450227700817816
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1685
epoch [2/100], loss:0.1267
epoch [3/100], loss:0.1162
epoch [4/100], loss:0.1009
epoch [5/100], loss:0.1024
epoch [6/100], loss:0.0881
epoch [7/100], loss:0.0938
epoch [8/100], loss:0.0846
epoch [9/100], loss:0.0789
epoch [10/100], loss:0.0775
epoch [11/100], loss:0.0802
epoch [12/100], loss:0.0826
epoch [13/100], loss:0.0762
epoch [14/100], loss:0.0730
epoch [15/100], loss:0.0761
epoch [16/100], loss:0.0733
epoch [17/100], loss:0.0745
epoch [18/100], loss:0.0723
epoch [19/100], loss:0.0751
epoch [20/100], loss:0.0676
epoch [21/100], loss:0.0689
epoch [22/100], loss:0.0672
epoch [23/100], loss:0.0645
epoch [24/100], loss:0.0686
epoch [25/100], loss:0.0595
epoch [26/100], loss:0.0708
epoch [27/100], loss:0.0627
epoch [28/100], loss:0.0600
epoch [29/100], loss:0.0733
epoch [30/100], loss:0.0682
epoch [31/100], loss:0.0678
epoch [32/100], loss:0.0643
epoch [33/100], loss:0.0609
epoch [34/100], loss:0.0668
epoch [35/100], loss:0.0597
epoch [36/100], loss:0.0654
epoch [37/100], loss:0.0577
epoch [38/100], loss:0.0635
epoch [39/100], loss:0.0612
epoch [40/100], loss:0.0613
epoch [41/100], loss:0.0553
epoch [42/100], loss:0.0607
epoch [43/100], loss:0.0553
epoch [44/100], loss:0.0569
epoch [45/100], loss:0.0538
epoch [46/100], loss:0.0614
epoch [47/100], loss:0.0612
epoch [48/100], loss:0.0612
epoch [49/100], loss:0.0542
epoch [50/100], loss:0.0562
epoch [51/100], loss:0.0626
epoch [52/100], loss:0.0572
epoch [53/100], loss:0.0573
epoch [54/100], loss:0.0582
epoch [55/100], loss:0.0580
epoch [56/100], loss:0.0548
epoch [57/100], loss:0.0573
epoch [58/100], loss:0.0572
epoch [59/100], loss:0.0602
epoch [60/100], loss:0.0596
epoch [61/100], loss:0.0564
epoch [62/100], loss:0.0558
epoch [63/100], loss:0.0642
epoch [64/100], loss:0.0534
epoch [65/100], loss:0.0588
epoch [66/100], loss:0.0467
epoch [67/100], loss:0.0503
epoch [68/100], loss:0.0533
epoch [69/100], loss:0.0542
epoch [70/100], loss:0.0563
epoch [71/100], loss:0.0549
epoch [72/100], loss:0.0594
epoch [73/100], loss:0.0551
epoch [74/100], loss:0.0560
epoch [75/100], loss:0.0565
epoch [76/100], loss:0.0527
epoch [77/100], loss:0.0585
epoch [78/100], loss:0.0537
epoch [79/100], loss:0.0559
epoch [80/100], loss:0.0640
epoch [81/100], loss:0.0532
epoch [82/100], loss:0.0529
epoch [83/100], loss:0.0511
epoch [84/100], loss:0.0555
epoch [85/100], loss:0.0540
epoch [86/100], loss:0.0566
epoch [87/100], loss:0.0516
epoch [88/100], loss:0.0539
epoch [89/100], loss:0.0496
epoch [90/100], loss:0.0499
epoch [91/100], loss:0.0490
epoch [92/100], loss:0.0531
epoch [93/100], loss:0.0522
epoch [94/100], loss:0.0539
epoch [95/100], loss:0.0489
epoch [96/100], loss:0.0563
epoch [97/100], loss:0.0517
epoch [98/100], loss:0.0523
epoch [99/100], loss:0.0515
epoch [100/100], loss:0.0505
Running pruning iteration 13
************pruning 95.601953488896 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 663 / 1152  0.5755208333333334
encoder.0.bias : 62 / 128  0.484375
encoder.2.weight : 128398 / 131072  0.9795989990234375
encoder.2.bias : 59 / 64  0.921875
encoder.4.weight : 7667 / 8192  0.9359130859375
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4083 / 4608  0.8860677083333334
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 1033 / 1152  0.8967013888888888
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8912 / 9216  0.9670138888888888
decoder.0.bias : 97 / 128  0.7578125
decoder.2.weight : 31101 / 32768  0.949127197265625
decoder.2.bias : 31 / 64  0.484375
decoder.4.weight : 7098 / 8192  0.866455078125
decoder.4.bias : 22 / 32  0.6875
decoder.6.weight : 11163 / 12800  0.872109375
decoder.6.bias : 11 / 16  0.6875
decoder.8.weight : 27 / 64  0.421875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 200482/209705 = 0.9560191697861281
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1718
epoch [2/100], loss:0.1441
epoch [3/100], loss:0.1232
epoch [4/100], loss:0.1158
epoch [5/100], loss:0.1083
epoch [6/100], loss:0.1197
epoch [7/100], loss:0.1040
epoch [8/100], loss:0.0988
epoch [9/100], loss:0.0992
epoch [10/100], loss:0.0936
epoch [11/100], loss:0.0938
epoch [12/100], loss:0.0866
epoch [13/100], loss:0.0938
epoch [14/100], loss:0.0883
epoch [15/100], loss:0.0891
epoch [16/100], loss:0.0860
epoch [17/100], loss:0.0937
epoch [18/100], loss:0.0938
epoch [19/100], loss:0.0800
epoch [20/100], loss:0.0748
epoch [21/100], loss:0.0799
epoch [22/100], loss:0.0826
epoch [23/100], loss:0.0812
epoch [24/100], loss:0.0850
epoch [25/100], loss:0.0834
epoch [26/100], loss:0.0795
epoch [27/100], loss:0.0778
epoch [28/100], loss:0.0813
epoch [29/100], loss:0.0771
epoch [30/100], loss:0.0775
epoch [31/100], loss:0.0728
epoch [32/100], loss:0.0708
epoch [33/100], loss:0.0755
epoch [34/100], loss:0.0704
epoch [35/100], loss:0.0773
epoch [36/100], loss:0.0652
epoch [37/100], loss:0.0747
epoch [38/100], loss:0.0760
epoch [39/100], loss:0.0703
epoch [40/100], loss:0.0761
epoch [41/100], loss:0.0749
epoch [42/100], loss:0.0644
epoch [43/100], loss:0.0715
epoch [44/100], loss:0.0669
epoch [45/100], loss:0.0684
epoch [46/100], loss:0.0729
epoch [47/100], loss:0.0695
epoch [48/100], loss:0.0641
epoch [49/100], loss:0.0707
epoch [50/100], loss:0.0713
epoch [51/100], loss:0.0674
epoch [52/100], loss:0.0638
epoch [53/100], loss:0.0694
epoch [54/100], loss:0.0710
epoch [55/100], loss:0.0678
epoch [56/100], loss:0.0680
epoch [57/100], loss:0.0667
epoch [58/100], loss:0.0614
epoch [59/100], loss:0.0656
epoch [60/100], loss:0.0596
epoch [61/100], loss:0.0571
epoch [62/100], loss:0.0638
epoch [63/100], loss:0.0676
epoch [64/100], loss:0.0648
epoch [65/100], loss:0.0649
epoch [66/100], loss:0.0626
epoch [67/100], loss:0.0595
epoch [68/100], loss:0.0698
epoch [69/100], loss:0.0627
epoch [70/100], loss:0.0616
epoch [71/100], loss:0.0667
epoch [72/100], loss:0.0609
epoch [73/100], loss:0.0614
epoch [74/100], loss:0.0577
epoch [75/100], loss:0.0562
epoch [76/100], loss:0.0616
epoch [77/100], loss:0.0551
epoch [78/100], loss:0.0604
epoch [79/100], loss:0.0594
epoch [80/100], loss:0.0611
epoch [81/100], loss:0.0590
epoch [82/100], loss:0.0584
epoch [83/100], loss:0.0582
epoch [84/100], loss:0.0573
epoch [85/100], loss:0.0572
epoch [86/100], loss:0.0608
epoch [87/100], loss:0.0621
epoch [88/100], loss:0.0590
epoch [89/100], loss:0.0578
epoch [90/100], loss:0.0524
epoch [91/100], loss:0.0555
epoch [92/100], loss:0.0592
epoch [93/100], loss:0.0563
epoch [94/100], loss:0.0580
epoch [95/100], loss:0.0560
epoch [96/100], loss:0.0581
epoch [97/100], loss:0.0499
epoch [98/100], loss:0.0623
epoch [99/100], loss:0.0605
epoch [100/100], loss:0.0537
Running pruning iteration 14
************pruning 96.4815627911168 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 697 / 1152  0.6050347222222222
encoder.0.bias : 67 / 128  0.5234375
encoder.2.weight : 128856 / 131072  0.98309326171875
encoder.2.bias : 59 / 64  0.921875
encoder.4.weight : 7726 / 8192  0.943115234375
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4201 / 4608  0.9116753472222222
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 1069 / 1152  0.9279513888888888
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 9017 / 9216  0.9784071180555556
decoder.0.bias : 105 / 128  0.8203125
decoder.2.weight : 31621 / 32768  0.964996337890625
decoder.2.bias : 36 / 64  0.5625
decoder.4.weight : 7280 / 8192  0.888671875
decoder.4.bias : 23 / 32  0.71875
decoder.6.weight : 11471 / 12800  0.896171875
decoder.6.bias : 13 / 16  0.8125
decoder.8.weight : 30 / 64  0.46875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 202326/209705 = 0.9648124746667939
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1853
epoch [2/100], loss:0.1584
epoch [3/100], loss:0.1233
epoch [4/100], loss:0.1211
epoch [5/100], loss:0.1202
epoch [6/100], loss:0.1144
epoch [7/100], loss:0.1126
epoch [8/100], loss:0.1088
epoch [9/100], loss:0.1092
epoch [10/100], loss:0.1122
epoch [11/100], loss:0.1082
epoch [12/100], loss:0.1084
epoch [13/100], loss:0.1056
epoch [14/100], loss:0.1059
epoch [15/100], loss:0.1009
epoch [16/100], loss:0.0908
epoch [17/100], loss:0.1096
epoch [18/100], loss:0.0941
epoch [19/100], loss:0.0978
epoch [20/100], loss:0.1022
epoch [21/100], loss:0.0901
epoch [22/100], loss:0.0966
epoch [23/100], loss:0.0956
epoch [24/100], loss:0.0837
epoch [25/100], loss:0.1034
epoch [26/100], loss:0.1007
epoch [27/100], loss:0.0917
epoch [28/100], loss:0.0901
epoch [29/100], loss:0.0820
epoch [30/100], loss:0.0866
epoch [31/100], loss:0.0819
epoch [32/100], loss:0.0887
epoch [33/100], loss:0.0894
epoch [34/100], loss:0.0916
epoch [35/100], loss:0.0820
epoch [36/100], loss:0.0882
epoch [37/100], loss:0.0871
epoch [38/100], loss:0.0827
epoch [39/100], loss:0.0842
epoch [40/100], loss:0.0834
epoch [41/100], loss:0.0855
epoch [42/100], loss:0.0872
epoch [43/100], loss:0.0820
epoch [44/100], loss:0.0851
epoch [45/100], loss:0.0711
epoch [46/100], loss:0.0794
epoch [47/100], loss:0.0786
epoch [48/100], loss:0.0772
epoch [49/100], loss:0.0788
epoch [50/100], loss:0.0806
epoch [51/100], loss:0.0850
epoch [52/100], loss:0.0770
epoch [53/100], loss:0.0783
epoch [54/100], loss:0.0824
epoch [55/100], loss:0.0750
epoch [56/100], loss:0.0760
epoch [57/100], loss:0.0761
epoch [58/100], loss:0.0868
epoch [59/100], loss:0.0851
epoch [60/100], loss:0.0727
epoch [61/100], loss:0.0740
epoch [62/100], loss:0.0777
epoch [63/100], loss:0.0731
epoch [64/100], loss:0.0781
epoch [65/100], loss:0.0807
epoch [66/100], loss:0.0821
epoch [67/100], loss:0.0762
epoch [68/100], loss:0.0753
epoch [69/100], loss:0.0752
epoch [70/100], loss:0.0816
epoch [71/100], loss:0.0775
epoch [72/100], loss:0.0707
epoch [73/100], loss:0.0690
epoch [74/100], loss:0.0780
epoch [75/100], loss:0.0696
epoch [76/100], loss:0.0734
epoch [77/100], loss:0.0757
epoch [78/100], loss:0.0700
epoch [79/100], loss:0.0684
epoch [80/100], loss:0.0741
epoch [81/100], loss:0.0778
epoch [82/100], loss:0.0746
epoch [83/100], loss:0.0699
epoch [84/100], loss:0.0739
epoch [85/100], loss:0.0676
epoch [86/100], loss:0.0687
epoch [87/100], loss:0.0719
epoch [88/100], loss:0.0752
epoch [89/100], loss:0.0723
epoch [90/100], loss:0.0740
epoch [91/100], loss:0.0717
epoch [92/100], loss:0.0711
epoch [93/100], loss:0.0704
epoch [94/100], loss:0.0664
epoch [95/100], loss:0.0666
epoch [96/100], loss:0.0698
epoch [97/100], loss:0.0744
epoch [98/100], loss:0.0690
epoch [99/100], loss:0.0672
epoch [100/100], loss:0.0728
Running pruning iteration 15
************pruning 97.18525023289344 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 742 / 1152  0.6440972222222222
encoder.0.bias : 70 / 128  0.546875
encoder.2.weight : 129301 / 131072  0.9864883422851562
encoder.2.bias : 59 / 64  0.921875
encoder.4.weight : 7831 / 8192  0.9559326171875
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4270 / 4608  0.9266493055555556
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 1085 / 1152  0.9418402777777778
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 9069 / 9216  0.9840494791666666
decoder.0.bias : 107 / 128  0.8359375
decoder.2.weight : 31991 / 32768  0.976287841796875
decoder.2.bias : 38 / 64  0.59375
decoder.4.weight : 7424 / 8192  0.90625
decoder.4.bias : 23 / 32  0.71875
decoder.6.weight : 11694 / 12800  0.91359375
decoder.6.bias : 13 / 16  0.8125
decoder.8.weight : 30 / 64  0.46875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 203802/209705 = 0.971850933454138
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.2445
epoch [2/100], loss:0.2332
epoch [3/100], loss:0.2297
epoch [4/100], loss:0.2321
epoch [5/100], loss:0.2276
epoch [6/100], loss:0.2258
epoch [7/100], loss:0.2226
epoch [8/100], loss:0.2260
epoch [9/100], loss:0.2255
epoch [10/100], loss:0.2302
epoch [11/100], loss:0.2258
epoch [12/100], loss:0.2310
epoch [13/100], loss:0.2240
epoch [14/100], loss:0.2181
epoch [15/100], loss:0.2169
epoch [16/100], loss:0.2276
epoch [17/100], loss:0.2250
epoch [18/100], loss:0.2269
epoch [19/100], loss:0.2247
epoch [20/100], loss:0.2291
epoch [21/100], loss:0.2178
epoch [22/100], loss:0.2213
epoch [23/100], loss:0.2175
epoch [24/100], loss:0.2350
epoch [25/100], loss:0.2351
epoch [26/100], loss:0.2357
epoch [27/100], loss:0.2381
epoch [28/100], loss:0.2139
epoch [29/100], loss:0.2182
epoch [30/100], loss:0.2204
epoch [31/100], loss:0.2339
epoch [32/100], loss:0.2214
epoch [33/100], loss:0.2274
epoch [34/100], loss:0.2222
epoch [35/100], loss:0.2257
epoch [36/100], loss:0.2379
epoch [37/100], loss:0.2234
epoch [38/100], loss:0.2332
epoch [39/100], loss:0.2097
epoch [40/100], loss:0.2204
epoch [41/100], loss:0.2223
epoch [42/100], loss:0.2215
epoch [43/100], loss:0.2416
epoch [44/100], loss:0.2356
epoch [45/100], loss:0.2290
epoch [46/100], loss:0.2222
epoch [47/100], loss:0.2251
epoch [48/100], loss:0.2286
epoch [49/100], loss:0.2216
epoch [50/100], loss:0.2165
epoch [51/100], loss:0.2312
epoch [52/100], loss:0.2210
epoch [53/100], loss:0.2245
epoch [54/100], loss:0.2221
epoch [55/100], loss:0.2264
epoch [56/100], loss:0.2158
epoch [57/100], loss:0.2201
epoch [58/100], loss:0.2238
epoch [59/100], loss:0.2292
epoch [60/100], loss:0.2264
epoch [61/100], loss:0.2174
epoch [62/100], loss:0.2134
epoch [63/100], loss:0.2265
epoch [64/100], loss:0.2263
epoch [65/100], loss:0.2261
epoch [66/100], loss:0.2348
epoch [67/100], loss:0.2273
epoch [68/100], loss:0.2150
epoch [69/100], loss:0.2313
epoch [70/100], loss:0.2380
epoch [71/100], loss:0.2241
epoch [72/100], loss:0.2252
epoch [73/100], loss:0.2137
epoch [74/100], loss:0.2198
epoch [75/100], loss:0.2294
epoch [76/100], loss:0.2307
epoch [77/100], loss:0.2262
epoch [78/100], loss:0.2181
epoch [79/100], loss:0.2201
epoch [80/100], loss:0.2246
epoch [81/100], loss:0.2330
epoch [82/100], loss:0.2210
epoch [83/100], loss:0.2279
epoch [84/100], loss:0.2170
epoch [85/100], loss:0.2296
epoch [86/100], loss:0.2154
epoch [87/100], loss:0.2388
epoch [88/100], loss:0.2250
epoch [89/100], loss:0.2338
epoch [90/100], loss:0.2262
epoch [91/100], loss:0.2220
epoch [92/100], loss:0.2341
epoch [93/100], loss:0.2255
epoch [94/100], loss:0.2297
epoch [95/100], loss:0.2282
epoch [96/100], loss:0.2322
epoch [97/100], loss:0.2205
epoch [98/100], loss:0.2285
epoch [99/100], loss:0.2388
epoch [100/100], loss:0.2243
Running pruning iteration 16
************pruning 97.74820018631475 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 903 / 1152  0.7838541666666666
encoder.0.bias : 92 / 128  0.71875
encoder.2.weight : 129779 / 131072  0.9901351928710938
encoder.2.bias : 60 / 64  0.9375
encoder.4.weight : 7946 / 8192  0.969970703125
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4373 / 4608  0.9490017361111112
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 1104 / 1152  0.9583333333333334
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 9080 / 9216  0.9852430555555556
decoder.0.bias : 107 / 128  0.8359375
decoder.2.weight : 32070 / 32768  0.97869873046875
decoder.2.bias : 38 / 64  0.59375
decoder.4.weight : 7496 / 8192  0.9150390625
decoder.4.bias : 23 / 32  0.71875
decoder.6.weight : 11812 / 12800  0.9228125
decoder.6.bias : 13 / 16  0.8125
decoder.8.weight : 31 / 64  0.484375
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 204982/209705 = 0.9774778856012016
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.2574
epoch [2/100], loss:0.2347
epoch [3/100], loss:0.2300
epoch [4/100], loss:0.2267
epoch [5/100], loss:0.2280
epoch [6/100], loss:0.2290
epoch [7/100], loss:0.2258
epoch [8/100], loss:0.2367
epoch [9/100], loss:0.2308
epoch [10/100], loss:0.2224
epoch [11/100], loss:0.2262
epoch [12/100], loss:0.2268
epoch [13/100], loss:0.2245
epoch [14/100], loss:0.2281
epoch [15/100], loss:0.2243
epoch [16/100], loss:0.2241
epoch [17/100], loss:0.2309
epoch [18/100], loss:0.2234
epoch [19/100], loss:0.2267
epoch [20/100], loss:0.2214
epoch [21/100], loss:0.2240
epoch [22/100], loss:0.2202
epoch [23/100], loss:0.2340
epoch [24/100], loss:0.2228
epoch [25/100], loss:0.2405
epoch [26/100], loss:0.2371
epoch [27/100], loss:0.2195
epoch [28/100], loss:0.2299
epoch [29/100], loss:0.2240
epoch [30/100], loss:0.2337
epoch [31/100], loss:0.2369
epoch [32/100], loss:0.2391
epoch [33/100], loss:0.2258
epoch [34/100], loss:0.2313
epoch [35/100], loss:0.2226
epoch [36/100], loss:0.2351
epoch [37/100], loss:0.2302
epoch [38/100], loss:0.2277
epoch [39/100], loss:0.2215
epoch [40/100], loss:0.2353
epoch [41/100], loss:0.2369
epoch [42/100], loss:0.2182
epoch [43/100], loss:0.2160
epoch [44/100], loss:0.2245
epoch [45/100], loss:0.2373
epoch [46/100], loss:0.2156
epoch [47/100], loss:0.2304
epoch [48/100], loss:0.2164
epoch [49/100], loss:0.2317
epoch [50/100], loss:0.2370
epoch [51/100], loss:0.2314
epoch [52/100], loss:0.2180
epoch [53/100], loss:0.2216
epoch [54/100], loss:0.2270
epoch [55/100], loss:0.2309
epoch [56/100], loss:0.2156
epoch [57/100], loss:0.2396
epoch [58/100], loss:0.2264
epoch [59/100], loss:0.2273
epoch [60/100], loss:0.2173
epoch [61/100], loss:0.2297
epoch [62/100], loss:0.2145
epoch [63/100], loss:0.2240
epoch [64/100], loss:0.2146
epoch [65/100], loss:0.2393
epoch [66/100], loss:0.2232
epoch [67/100], loss:0.2358
epoch [68/100], loss:0.2159
epoch [69/100], loss:0.2324
epoch [70/100], loss:0.2340
epoch [71/100], loss:0.2271
epoch [72/100], loss:0.2129
epoch [73/100], loss:0.2256
epoch [74/100], loss:0.2296
epoch [75/100], loss:0.2316
epoch [76/100], loss:0.2289
epoch [77/100], loss:0.2210
epoch [78/100], loss:0.2252
epoch [79/100], loss:0.2303
epoch [80/100], loss:0.2339
epoch [81/100], loss:0.2283
epoch [82/100], loss:0.2274
epoch [83/100], loss:0.2235
epoch [84/100], loss:0.2342
epoch [85/100], loss:0.2286
epoch [86/100], loss:0.2295
epoch [87/100], loss:0.2248
epoch [88/100], loss:0.2166
epoch [89/100], loss:0.2285
epoch [90/100], loss:0.2152
epoch [91/100], loss:0.2347
epoch [92/100], loss:0.2255
epoch [93/100], loss:0.2287
epoch [94/100], loss:0.2204
epoch [95/100], loss:0.2219
epoch [96/100], loss:0.2149
epoch [97/100], loss:0.2172
epoch [98/100], loss:0.2313
epoch [99/100], loss:0.2225
epoch [100/100], loss:0.2157
Running pruning iteration 17
************pruning 98.1985601490518 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 1017 / 1152  0.8828125
encoder.0.bias : 108 / 128  0.84375
encoder.2.weight : 130180 / 131072  0.993194580078125
encoder.2.bias : 60 / 64  0.9375
encoder.4.weight : 8024 / 8192  0.9794921875
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4460 / 4608  0.9678819444444444
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 1121 / 1152  0.9730902777777778
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 9124 / 9216  0.9900173611111112
decoder.0.bias : 107 / 128  0.8359375
decoder.2.weight : 32168 / 32768  0.981689453125
decoder.2.bias : 38 / 64  0.59375
decoder.4.weight : 7545 / 8192  0.9210205078125
decoder.4.bias : 24 / 32  0.75
decoder.6.weight : 11852 / 12800  0.9259375
decoder.6.bias : 13 / 16  0.8125
decoder.8.weight : 31 / 64  0.484375
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 205927/209705 = 0.9819842159223672
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.2714
epoch [2/100], loss:0.2329
epoch [3/100], loss:0.2258
epoch [4/100], loss:0.2307
epoch [5/100], loss:0.2355
epoch [6/100], loss:0.2283
epoch [7/100], loss:0.2148
epoch [8/100], loss:0.2399
epoch [9/100], loss:0.2264
epoch [10/100], loss:0.2456
epoch [11/100], loss:0.2252
epoch [12/100], loss:0.2167
epoch [13/100], loss:0.2258
epoch [14/100], loss:0.2307
epoch [15/100], loss:0.2323
epoch [16/100], loss:0.2203
epoch [17/100], loss:0.2270
epoch [18/100], loss:0.2247
epoch [19/100], loss:0.2294
epoch [20/100], loss:0.2284
epoch [21/100], loss:0.2250
epoch [22/100], loss:0.2245
epoch [23/100], loss:0.2342
epoch [24/100], loss:0.2246
epoch [25/100], loss:0.2112
epoch [26/100], loss:0.2261
epoch [27/100], loss:0.2265
epoch [28/100], loss:0.2438
epoch [29/100], loss:0.2220
epoch [30/100], loss:0.2246
epoch [31/100], loss:0.2284
epoch [32/100], loss:0.2181
epoch [33/100], loss:0.2313
epoch [34/100], loss:0.2135
epoch [35/100], loss:0.2313
epoch [36/100], loss:0.2308
epoch [37/100], loss:0.2219
epoch [38/100], loss:0.2250
epoch [39/100], loss:0.2293
epoch [40/100], loss:0.2222
epoch [41/100], loss:0.2285
epoch [42/100], loss:0.2273
epoch [43/100], loss:0.2294
epoch [44/100], loss:0.2274
epoch [45/100], loss:0.2206
epoch [46/100], loss:0.2265
epoch [47/100], loss:0.2202
epoch [48/100], loss:0.2273
epoch [49/100], loss:0.2214
epoch [50/100], loss:0.2312
epoch [51/100], loss:0.2179
epoch [52/100], loss:0.2278
epoch [53/100], loss:0.2270
epoch [54/100], loss:0.2351
epoch [55/100], loss:0.2260
epoch [56/100], loss:0.2316
epoch [57/100], loss:0.2301
epoch [58/100], loss:0.2288
epoch [59/100], loss:0.2223
epoch [60/100], loss:0.2172
epoch [61/100], loss:0.2420
epoch [62/100], loss:0.2170
epoch [63/100], loss:0.2320
epoch [64/100], loss:0.2191
epoch [65/100], loss:0.2146
epoch [66/100], loss:0.2346
epoch [67/100], loss:0.2262
epoch [68/100], loss:0.2167
epoch [69/100], loss:0.2359
epoch [70/100], loss:0.2239
epoch [71/100], loss:0.2189
epoch [72/100], loss:0.2233
epoch [73/100], loss:0.2275
epoch [74/100], loss:0.2290
epoch [75/100], loss:0.2297
epoch [76/100], loss:0.2173
epoch [77/100], loss:0.2372
epoch [78/100], loss:0.2140
epoch [79/100], loss:0.2238
epoch [80/100], loss:0.2330
epoch [81/100], loss:0.2218
epoch [82/100], loss:0.2238
epoch [83/100], loss:0.2215
epoch [84/100], loss:0.2312
epoch [85/100], loss:0.2255
epoch [86/100], loss:0.2380
epoch [87/100], loss:0.2148
epoch [88/100], loss:0.2356
epoch [89/100], loss:0.2338
epoch [90/100], loss:0.2262
epoch [91/100], loss:0.2266
epoch [92/100], loss:0.2326
epoch [93/100], loss:0.2161
epoch [94/100], loss:0.2363
epoch [95/100], loss:0.2289
epoch [96/100], loss:0.2207
epoch [97/100], loss:0.2270
epoch [98/100], loss:0.2222
epoch [99/100], loss:0.2322
epoch [100/100], loss:0.2321
Running pruning iteration 18
************pruning 98.55884811924145 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 1094 / 1152  0.9496527777777778
encoder.0.bias : 121 / 128  0.9453125
encoder.2.weight : 130512 / 131072  0.9957275390625
encoder.2.bias : 62 / 64  0.96875
encoder.4.weight : 8093 / 8192  0.9879150390625
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4525 / 4608  0.9819878472222222
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 1139 / 1152  0.9887152777777778
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 9154 / 9216  0.9932725694444444
decoder.0.bias : 107 / 128  0.8359375
decoder.2.weight : 32252 / 32768  0.9842529296875
decoder.2.bias : 38 / 64  0.59375
decoder.4.weight : 7591 / 8192  0.9266357421875
decoder.4.bias : 24 / 32  0.75
decoder.6.weight : 11870 / 12800  0.92734375
decoder.6.bias : 13 / 16  0.8125
decoder.8.weight : 31 / 64  0.484375
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 206682/209705 = 0.9855845115757851
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.2686
epoch [2/100], loss:0.2305
epoch [3/100], loss:0.2288
epoch [4/100], loss:0.2246
epoch [5/100], loss:0.2293
epoch [6/100], loss:0.2357
epoch [7/100], loss:0.2247
epoch [8/100], loss:0.2309
epoch [9/100], loss:0.2354
epoch [10/100], loss:0.2311
epoch [11/100], loss:0.2185
epoch [12/100], loss:0.2234
epoch [13/100], loss:0.2203
epoch [14/100], loss:0.2227
epoch [15/100], loss:0.2187
epoch [16/100], loss:0.2223
epoch [17/100], loss:0.2195
epoch [18/100], loss:0.2220
epoch [19/100], loss:0.2354
epoch [20/100], loss:0.2308
epoch [21/100], loss:0.2323
epoch [22/100], loss:0.2266
epoch [23/100], loss:0.2307
epoch [24/100], loss:0.2336
epoch [25/100], loss:0.2311
epoch [26/100], loss:0.2258
epoch [27/100], loss:0.2291
epoch [28/100], loss:0.2299
epoch [29/100], loss:0.2233
epoch [30/100], loss:0.2170
epoch [31/100], loss:0.2229
epoch [32/100], loss:0.2296
epoch [33/100], loss:0.2261
epoch [34/100], loss:0.2187
epoch [35/100], loss:0.2264
epoch [36/100], loss:0.2189
epoch [37/100], loss:0.2101
epoch [38/100], loss:0.2263
epoch [39/100], loss:0.2158
epoch [40/100], loss:0.2256
epoch [41/100], loss:0.2319
epoch [42/100], loss:0.2264
epoch [43/100], loss:0.2214
epoch [44/100], loss:0.2145
epoch [45/100], loss:0.2230
epoch [46/100], loss:0.2218
epoch [47/100], loss:0.2201
epoch [48/100], loss:0.2345
epoch [49/100], loss:0.2200
epoch [50/100], loss:0.2280
epoch [51/100], loss:0.2166
epoch [52/100], loss:0.2325
epoch [53/100], loss:0.2231
epoch [54/100], loss:0.2290
epoch [55/100], loss:0.2280
epoch [56/100], loss:0.2188
epoch [57/100], loss:0.2392
epoch [58/100], loss:0.2296
epoch [59/100], loss:0.2231
epoch [60/100], loss:0.2377
epoch [61/100], loss:0.2269
epoch [62/100], loss:0.2174
epoch [63/100], loss:0.2339
epoch [64/100], loss:0.2164
epoch [65/100], loss:0.2207
epoch [66/100], loss:0.2255
epoch [67/100], loss:0.2291
epoch [68/100], loss:0.2207
epoch [69/100], loss:0.2181
epoch [70/100], loss:0.2317
epoch [71/100], loss:0.2195
epoch [72/100], loss:0.2249
epoch [73/100], loss:0.2288
epoch [74/100], loss:0.2407
epoch [75/100], loss:0.2307
epoch [76/100], loss:0.2264
epoch [77/100], loss:0.2204
epoch [78/100], loss:0.2286
epoch [79/100], loss:0.2285
epoch [80/100], loss:0.2235
epoch [81/100], loss:0.2246
epoch [82/100], loss:0.2379
epoch [83/100], loss:0.2304
epoch [84/100], loss:0.2266
epoch [85/100], loss:0.2287
epoch [86/100], loss:0.2151
epoch [87/100], loss:0.2229
epoch [88/100], loss:0.2274
epoch [89/100], loss:0.2351
epoch [90/100], loss:0.2226
epoch [91/100], loss:0.2246
epoch [92/100], loss:0.2181
epoch [93/100], loss:0.2257
epoch [94/100], loss:0.2213
epoch [95/100], loss:0.2229
epoch [96/100], loss:0.2265
epoch [97/100], loss:0.2239
epoch [98/100], loss:0.2340
epoch [99/100], loss:0.2327
epoch [100/100], loss:0.2387
Running pruning iteration 19
************pruning 98.84707849539316 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 1129 / 1152  0.9800347222222222
encoder.0.bias : 125 / 128  0.9765625
encoder.2.weight : 130770 / 131072  0.9976959228515625
encoder.2.bias : 63 / 64  0.984375
encoder.4.weight : 8139 / 8192  0.9935302734375
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4576 / 4608  0.9930555555555556
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 1145 / 1152  0.9939236111111112
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 9168 / 9216  0.9947916666666666
decoder.0.bias : 107 / 128  0.8359375
decoder.2.weight : 32313 / 32768  0.986114501953125
decoder.2.bias : 39 / 64  0.609375
decoder.4.weight : 7643 / 8192  0.9329833984375
decoder.4.bias : 24 / 32  0.75
decoder.6.weight : 11946 / 12800  0.93328125
decoder.6.bias : 13 / 16  0.8125
decoder.8.weight : 31 / 64  0.484375
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 207287/209705 = 0.9884695167020338
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.2605
epoch [2/100], loss:0.2271
epoch [3/100], loss:0.2250
epoch [4/100], loss:0.2240
epoch [5/100], loss:0.2259
epoch [6/100], loss:0.2287
epoch [7/100], loss:0.2299
epoch [8/100], loss:0.2110
epoch [9/100], loss:0.2336
epoch [10/100], loss:0.2354
epoch [11/100], loss:0.2259
epoch [12/100], loss:0.2255
epoch [13/100], loss:0.2276
epoch [14/100], loss:0.2267
epoch [15/100], loss:0.2178
epoch [16/100], loss:0.2498
epoch [17/100], loss:0.2210
epoch [18/100], loss:0.2179
epoch [19/100], loss:0.2201
epoch [20/100], loss:0.2205
epoch [21/100], loss:0.2242
epoch [22/100], loss:0.2213
epoch [23/100], loss:0.2170
epoch [24/100], loss:0.2380
epoch [25/100], loss:0.2349
epoch [26/100], loss:0.2225
epoch [27/100], loss:0.2372
epoch [28/100], loss:0.2121
epoch [29/100], loss:0.2246
epoch [30/100], loss:0.2211
epoch [31/100], loss:0.2201
epoch [32/100], loss:0.2344
epoch [33/100], loss:0.2269
epoch [34/100], loss:0.2272
epoch [35/100], loss:0.2214
epoch [36/100], loss:0.2421
epoch [37/100], loss:0.2241
epoch [38/100], loss:0.2208
epoch [39/100], loss:0.2259
epoch [40/100], loss:0.2145
epoch [41/100], loss:0.2294
epoch [42/100], loss:0.2340
epoch [43/100], loss:0.2324
epoch [44/100], loss:0.2223
epoch [45/100], loss:0.2370
epoch [46/100], loss:0.2223
epoch [47/100], loss:0.2365
epoch [48/100], loss:0.2120
epoch [49/100], loss:0.2282
epoch [50/100], loss:0.2242
epoch [51/100], loss:0.2208
epoch [52/100], loss:0.2345
epoch [53/100], loss:0.2179
epoch [54/100], loss:0.2267
epoch [55/100], loss:0.2317
epoch [56/100], loss:0.2341
epoch [57/100], loss:0.2212
epoch [58/100], loss:0.2180
epoch [59/100], loss:0.2239
epoch [60/100], loss:0.2204
epoch [61/100], loss:0.2159
epoch [62/100], loss:0.2257
epoch [63/100], loss:0.2513
epoch [64/100], loss:0.2136
epoch [65/100], loss:0.2240
epoch [66/100], loss:0.2367
epoch [67/100], loss:0.2226
epoch [68/100], loss:0.2396
epoch [69/100], loss:0.2435
epoch [70/100], loss:0.2235
epoch [71/100], loss:0.2285
epoch [72/100], loss:0.2329
epoch [73/100], loss:0.2163
epoch [74/100], loss:0.2323
epoch [75/100], loss:0.2228
epoch [76/100], loss:0.2374
epoch [77/100], loss:0.2248
epoch [78/100], loss:0.2208
epoch [79/100], loss:0.2290
epoch [80/100], loss:0.2174
epoch [81/100], loss:0.2178
epoch [82/100], loss:0.2313
epoch [83/100], loss:0.2319
epoch [84/100], loss:0.2233
epoch [85/100], loss:0.2221
epoch [86/100], loss:0.2242
epoch [87/100], loss:0.2269
epoch [88/100], loss:0.2256
epoch [89/100], loss:0.2206
epoch [90/100], loss:0.2205
epoch [91/100], loss:0.2200
epoch [92/100], loss:0.2279
epoch [93/100], loss:0.2306
epoch [94/100], loss:0.2386
epoch [95/100], loss:0.2177
epoch [96/100], loss:0.2272
epoch [97/100], loss:0.2174
epoch [98/100], loss:0.2364
epoch [99/100], loss:0.2268
epoch [100/100], loss:0.2262
Finished Iterative Pruning
