tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,
        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,
        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,
        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,
        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,
        2, 0, 2, 7, 1, 8, 6, 4])
[20.0, 16.0, 12.8, 10.240000000000002, 8.192, 6.5536, 5.2428799999999995, 4.194304, 3.3554431999999994, 2.68435456, 2.1474836479999992, 1.7179869183999985, 1.3743895347199981, 1.099511627775999, 0.8796093022207998, 0.7036874417766399, 0.5629499534213125, 0.45035996273705053, 0.3602879701896399, 0.28823037615171077, 0.23058430092136747]
Weight fractions: [20.0, 36.0, 48.8, 59.04, 67.232, 73.7856, 79.02848, 83.222784, 86.5782272, 89.26258176, 91.41006540800001, 93.12805232640001, 94.50244186112, 95.601953488896, 96.4815627911168, 97.18525023289344, 97.74820018631475, 98.1985601490518, 98.55884811924145, 98.84707849539316]
***************Iterative Pruning started. Number of iterations: 20 *****************
Running pruning iteration 0
************pruning 20.0 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 2 / 1152  0.001736111111111111
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 34348 / 131072  0.262054443359375
encoder.2.bias : 8 / 64  0.125
encoder.4.weight : 743 / 8192  0.0906982421875
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 159 / 4608  0.034505208333333336
encoder.6.bias : 1 / 16  0.0625
encoder.9.weight : 9 / 1152  0.0078125
encoder.9.bias : 0 / 8  0.0
decoder.0.weight : 840 / 9216  0.09114583333333333
decoder.0.bias : 0 / 128  0.0
decoder.2.weight : 3493 / 32768  0.106597900390625
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 962 / 8192  0.117431640625
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 1372 / 12800  0.1071875
decoder.6.bias : 0 / 16  0.0
decoder.8.weight : 4 / 64  0.0625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 41941/209705 = 0.2
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.3835
epoch [2/100], loss:0.3882
epoch [3/100], loss:0.4132
epoch [4/100], loss:0.4011
epoch [5/100], loss:0.4140
epoch [6/100], loss:0.4345
epoch [7/100], loss:0.4107
epoch [8/100], loss:0.3923
epoch [9/100], loss:0.4064
epoch [10/100], loss:0.1345
epoch [11/100], loss:0.0767
epoch [12/100], loss:0.0640
epoch [13/100], loss:0.0552
epoch [14/100], loss:0.0483
epoch [15/100], loss:0.0394
epoch [16/100], loss:0.0426
epoch [17/100], loss:0.0379
epoch [18/100], loss:0.0355
epoch [19/100], loss:0.0367
epoch [20/100], loss:0.0335
epoch [21/100], loss:0.0328
epoch [22/100], loss:0.0343
epoch [23/100], loss:0.0336
epoch [24/100], loss:0.0310
epoch [25/100], loss:0.0322
epoch [26/100], loss:0.0298
epoch [27/100], loss:0.0280
epoch [28/100], loss:0.0285
epoch [29/100], loss:0.0289
epoch [30/100], loss:0.0269
epoch [31/100], loss:0.0346
epoch [32/100], loss:0.0262
epoch [33/100], loss:0.0283
epoch [34/100], loss:0.0270
epoch [35/100], loss:0.0245
epoch [36/100], loss:0.0288
epoch [37/100], loss:0.0282
epoch [38/100], loss:0.0245
epoch [39/100], loss:0.0271
epoch [40/100], loss:0.0246
epoch [41/100], loss:0.0242
epoch [42/100], loss:0.0252
epoch [43/100], loss:0.0209
epoch [44/100], loss:0.0227
epoch [45/100], loss:0.0201
epoch [46/100], loss:0.0210
epoch [47/100], loss:0.0201
epoch [48/100], loss:0.0224
epoch [49/100], loss:0.0203
epoch [50/100], loss:0.0246
epoch [51/100], loss:0.0236
epoch [52/100], loss:0.0233
epoch [53/100], loss:0.0229
epoch [54/100], loss:0.0197
epoch [55/100], loss:0.0223
epoch [56/100], loss:0.0226
epoch [57/100], loss:0.0209
epoch [58/100], loss:0.0222
epoch [59/100], loss:0.0210
epoch [60/100], loss:0.0204
epoch [61/100], loss:0.0202
epoch [62/100], loss:0.0212
epoch [63/100], loss:0.0201
epoch [64/100], loss:0.0191
epoch [65/100], loss:0.0198
epoch [66/100], loss:0.0183
epoch [67/100], loss:0.0201
epoch [68/100], loss:0.0199
epoch [69/100], loss:0.0201
epoch [70/100], loss:0.0204
epoch [71/100], loss:0.0199
epoch [72/100], loss:0.0196
epoch [73/100], loss:0.0191
epoch [74/100], loss:0.0188
epoch [75/100], loss:0.0191
epoch [76/100], loss:0.0207
epoch [77/100], loss:0.0206
epoch [78/100], loss:0.0212
epoch [79/100], loss:0.0180
epoch [80/100], loss:0.0210
epoch [81/100], loss:0.0177
epoch [82/100], loss:0.0191
epoch [83/100], loss:0.0208
epoch [84/100], loss:0.0212
epoch [85/100], loss:0.0171
epoch [86/100], loss:0.0187
epoch [87/100], loss:0.0193
epoch [88/100], loss:0.0171
epoch [89/100], loss:0.0177
epoch [90/100], loss:0.0191
epoch [91/100], loss:0.0205
epoch [92/100], loss:0.0192
epoch [93/100], loss:0.0208
epoch [94/100], loss:0.0177
epoch [95/100], loss:0.0208
epoch [96/100], loss:0.0206
epoch [97/100], loss:0.0184
epoch [98/100], loss:0.0193
epoch [99/100], loss:0.0201
epoch [100/100], loss:0.0188
Running pruning iteration 1
************pruning 36.0 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 2 / 1152  0.001736111111111111
encoder.0.bias : 0 / 128  0.0
encoder.2.weight : 61290 / 131072  0.4676055908203125
encoder.2.bias : 14 / 64  0.21875
encoder.4.weight : 1770 / 8192  0.216064453125
encoder.4.bias : 0 / 32  0.0
encoder.6.weight : 384 / 4608  0.08333333333333333
encoder.6.bias : 1 / 16  0.0625
encoder.9.weight : 44 / 1152  0.03819444444444445
encoder.9.bias : 0 / 8  0.0
decoder.0.weight : 1675 / 9216  0.18174913194444445
decoder.0.bias : 1 / 128  0.0078125
decoder.2.weight : 6762 / 32768  0.20635986328125
decoder.2.bias : 0 / 64  0.0
decoder.4.weight : 1629 / 8192  0.1988525390625
decoder.4.bias : 0 / 32  0.0
decoder.6.weight : 1913 / 12800  0.149453125
decoder.6.bias : 0 / 16  0.0
decoder.8.weight : 9 / 64  0.140625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 75494/209705 = 0.3600009537207029
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.3967
epoch [2/100], loss:0.3963
epoch [3/100], loss:0.4219
epoch [4/100], loss:0.3869
epoch [5/100], loss:0.4079
epoch [6/100], loss:0.4385
epoch [7/100], loss:0.2129
epoch [8/100], loss:0.1255
epoch [9/100], loss:0.1061
epoch [10/100], loss:0.0914
epoch [11/100], loss:0.0904
epoch [12/100], loss:0.0885
epoch [13/100], loss:0.0794
epoch [14/100], loss:0.0874
epoch [15/100], loss:0.0723
epoch [16/100], loss:0.0742
epoch [17/100], loss:0.0701
epoch [18/100], loss:0.0741
epoch [19/100], loss:0.0756
epoch [20/100], loss:0.0720
epoch [21/100], loss:0.0739
epoch [22/100], loss:0.0708
epoch [23/100], loss:0.0691
epoch [24/100], loss:0.0669
epoch [25/100], loss:0.0718
epoch [26/100], loss:0.0615
epoch [27/100], loss:0.0689
epoch [28/100], loss:0.0652
epoch [29/100], loss:0.0687
epoch [30/100], loss:0.0681
epoch [31/100], loss:0.0591
epoch [32/100], loss:0.0656
epoch [33/100], loss:0.0554
epoch [34/100], loss:0.0722
epoch [35/100], loss:0.0638
epoch [36/100], loss:0.0629
epoch [37/100], loss:0.0632
epoch [38/100], loss:0.0661
epoch [39/100], loss:0.0696
epoch [40/100], loss:0.0577
epoch [41/100], loss:0.0627
epoch [42/100], loss:0.0620
epoch [43/100], loss:0.0524
epoch [44/100], loss:0.0621
epoch [45/100], loss:0.0661
epoch [46/100], loss:0.0601
epoch [47/100], loss:0.0533
epoch [48/100], loss:0.0627
epoch [49/100], loss:0.0579
epoch [50/100], loss:0.0515
epoch [51/100], loss:0.0608
epoch [52/100], loss:0.0600
epoch [53/100], loss:0.0532
epoch [54/100], loss:0.0554
epoch [55/100], loss:0.0565
epoch [56/100], loss:0.0548
epoch [57/100], loss:0.0536
epoch [58/100], loss:0.0500
epoch [59/100], loss:0.0494
epoch [60/100], loss:0.0507
epoch [61/100], loss:0.0573
epoch [62/100], loss:0.0492
epoch [63/100], loss:0.0517
epoch [64/100], loss:0.0513
epoch [65/100], loss:0.0523
epoch [66/100], loss:0.0525
epoch [67/100], loss:0.0487
epoch [68/100], loss:0.0439
epoch [69/100], loss:0.0491
epoch [70/100], loss:0.0477
epoch [71/100], loss:0.0477
epoch [72/100], loss:0.0505
epoch [73/100], loss:0.0478
epoch [74/100], loss:0.0485
epoch [75/100], loss:0.0526
