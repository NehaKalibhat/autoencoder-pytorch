Logging************pruning 89.26258176 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 454 / 1152  0.3940972222222222
encoder.0.bias : 51 / 128  0.3984375
encoder.2.weight : 124225 / 131072  0.9477615356445312
encoder.2.bias : 56 / 64  0.875
encoder.4.weight : 7253 / 8192  0.8853759765625
encoder.4.bias : 28 / 32  0.875
encoder.6.weight : 3349 / 4608  0.7267795138888888
encoder.6.bias : 10 / 16  0.625
encoder.9.weight : 839 / 1152  0.7282986111111112
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8315 / 9216  0.9022352430555556
decoder.0.bias : 58 / 128  0.453125
decoder.2.weight : 27752 / 32768  0.846923828125
decoder.2.bias : 25 / 64  0.390625
decoder.4.weight : 6340 / 8192  0.77392578125
decoder.4.bias : 16 / 32  0.5
decoder.6.weight : 8385 / 12800  0.655078125
decoder.6.bias : 7 / 16  0.4375
decoder.8.weight : 18 / 64  0.28125
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 187188/209705 = 0.8926253546648864
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1874
epoch [2/100], loss:0.1156
epoch [3/100], loss:0.0938
epoch [4/100], loss:0.0713
epoch [5/100], loss:0.0772
epoch [6/100], loss:0.0695
epoch [7/100], loss:0.0652
epoch [8/100], loss:0.0630
epoch [9/100], loss:0.0699
epoch [10/100], loss:0.0635
epoch [11/100], loss:0.0594
epoch [12/100], loss:0.0547
epoch [13/100], loss:0.0539
epoch [14/100], loss:0.0532
epoch [15/100], loss:0.0597
epoch [16/100], loss:0.0513
