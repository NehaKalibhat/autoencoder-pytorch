Logging************pruning 96 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 734 / 1152  0.6371527777777778
encoder.0.bias : 73 / 128  0.5703125
encoder.2.weight : 130683 / 131072  0.9970321655273438
encoder.2.bias : 64 / 64  1.0
encoder.4.weight : 7635 / 8192  0.9320068359375
encoder.4.bias : 30 / 32  0.9375
encoder.6.weight : 3977 / 4608  0.8630642361111112
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 1009 / 1152  0.8758680555555556
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8269 / 9216  0.8972439236111112
decoder.0.bias : 104 / 128  0.8125
decoder.2.weight : 30198 / 32768  0.92156982421875
decoder.2.bias : 22 / 64  0.34375
decoder.4.weight : 6981 / 8192  0.8521728515625
decoder.4.bias : 11 / 32  0.34375
decoder.6.weight : 11480 / 12800  0.896875
decoder.6.bias : 10 / 16  0.625
decoder.8.weight : 12 / 64  0.1875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 201316/209705 = 0.9599961851171884
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1815
epoch [2/100], loss:0.1155
epoch [3/100], loss:0.1075
epoch [4/100], loss:0.0945
epoch [5/100], loss:0.0834
epoch [6/100], loss:0.0871
epoch [7/100], loss:0.0913
epoch [8/100], loss:0.0755
epoch [9/100], loss:0.0872
epoch [10/100], loss:0.0799
epoch [11/100], loss:0.0642
epoch [12/100], loss:0.0796
epoch [13/100], loss:0.0799
epoch [14/100], loss:0.0729
epoch [15/100], loss:0.0696
epoch [16/100], loss:0.0745
epoch [17/100], loss:0.0716
epoch [18/100], loss:0.0751
epoch [19/100], loss:0.0683
epoch [20/100], loss:0.0659
epoch [21/100], loss:0.0617
epoch [22/100], loss:0.0642
epoch [23/100], loss:0.0676
epoch [24/100], loss:0.0676
epoch [25/100], loss:0.0631
epoch [26/100], loss:0.0578
epoch [27/100], loss:0.0665
epoch [28/100], loss:0.0671
epoch [29/100], loss:0.0568
epoch [30/100], loss:0.0584
epoch [31/100], loss:0.0606
epoch [32/100], loss:0.0574
epoch [33/100], loss:0.0571
epoch [34/100], loss:0.0625
epoch [35/100], loss:0.0648
epoch [36/100], loss:0.0630
epoch [37/100], loss:0.0564
epoch [38/100], loss:0.0585
epoch [39/100], loss:0.0586
epoch [40/100], loss:0.0589
epoch [41/100], loss:0.0521
epoch [42/100], loss:0.0606
epoch [43/100], loss:0.0570
epoch [44/100], loss:0.0610
epoch [45/100], loss:0.0570
epoch [46/100], loss:0.0568
epoch [47/100], loss:0.0551
epoch [48/100], loss:0.0533
epoch [49/100], loss:0.0590
epoch [50/100], loss:0.0516
epoch [51/100], loss:0.0555
epoch [52/100], loss:0.0559
epoch [53/100], loss:0.0550
epoch [54/100], loss:0.0574
epoch [55/100], loss:0.0490
epoch [56/100], loss:0.0539
epoch [57/100], loss:0.0533
epoch [58/100], loss:0.0516
epoch [59/100], loss:0.0567
epoch [60/100], loss:0.0508
epoch [61/100], loss:0.0571
epoch [62/100], loss:0.0568
epoch [63/100], loss:0.0525
epoch [64/100], loss:0.0554
epoch [65/100], loss:0.0519
epoch [66/100], loss:0.0516
epoch [67/100], loss:0.0542
epoch [68/100], loss:0.0528
epoch [69/100], loss:0.0552
epoch [70/100], loss:0.0520
epoch [71/100], loss:0.0536
epoch [72/100], loss:0.0510
epoch [73/100], loss:0.0523
epoch [74/100], loss:0.0543
epoch [75/100], loss:0.0497
epoch [76/100], loss:0.0515
epoch [77/100], loss:0.0507
epoch [78/100], loss:0.0470
epoch [79/100], loss:0.0562
epoch [80/100], loss:0.0439
epoch [81/100], loss:0.0492
epoch [82/100], loss:0.0479
epoch [83/100], loss:0.0477
epoch [84/100], loss:0.0495
epoch [85/100], loss:0.0574
epoch [86/100], loss:0.0519
epoch [87/100], loss:0.0490
epoch [88/100], loss:0.0503
epoch [89/100], loss:0.0464
epoch [90/100], loss:0.0556
epoch [91/100], loss:0.0495
epoch [92/100], loss:0.0493
epoch [93/100], loss:0.0447
epoch [94/100], loss:0.0511
epoch [95/100], loss:0.0499
epoch [96/100], loss:0.0477
epoch [97/100], loss:0.0501
epoch [98/100], loss:0.0517
epoch [99/100], loss:0.0478
epoch [100/100], loss:0.0528
