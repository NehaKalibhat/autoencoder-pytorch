Logging************pruning 80 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 1301 / 4608  0.2823350694444444
encoder.0.bias : 111 / 512  0.216796875
encoder.2.weight : 1761240 / 2097152  0.8398246765136719
encoder.2.bias : 136 / 256  0.53125
encoder.4.weight : 119556 / 131072  0.912139892578125
encoder.4.bias : 71 / 128  0.5546875
encoder.6.weight : 57092 / 73728  0.7743598090277778
encoder.6.bias : 18 / 64  0.28125
encoder.9.weight : 7874 / 18432  0.4271918402777778
encoder.9.bias : 6 / 32  0.1875
decoder.0.weight : 70370 / 147456  0.4772271050347222
decoder.0.bias : 107 / 512  0.208984375
decoder.2.weight : 426320 / 524288  0.813140869140625
decoder.2.bias : 82 / 256  0.3203125
decoder.4.weight : 102942 / 131072  0.7853851318359375
decoder.4.bias : 22 / 128  0.171875
decoder.6.weight : 120520 / 204800  0.5884765625
decoder.6.bias : 17 / 64  0.265625
decoder.8.weight : 68 / 256  0.265625
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 2667853/3334817 = 0.7999998200800823
Number of parameters in model 3334817
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.0580
epoch [2/100], loss:0.0323
epoch [3/100], loss:0.0310
epoch [4/100], loss:0.0250
epoch [5/100], loss:0.0202
epoch [6/100], loss:0.0222
epoch [7/100], loss:0.0186
epoch [8/100], loss:0.0165
epoch [9/100], loss:0.0150
epoch [10/100], loss:0.0152
epoch [11/100], loss:0.0143
epoch [12/100], loss:0.0136
epoch [13/100], loss:0.0135
epoch [14/100], loss:0.0117
epoch [15/100], loss:0.0104
epoch [16/100], loss:0.0120
epoch [17/100], loss:0.0109
epoch [18/100], loss:0.0108
epoch [19/100], loss:0.0108
epoch [20/100], loss:0.0111
epoch [21/100], loss:0.0105
epoch [22/100], loss:0.0104
epoch [23/100], loss:0.0106
epoch [24/100], loss:0.0090
epoch [25/100], loss:0.0094
epoch [26/100], loss:0.0087
epoch [27/100], loss:0.0086
epoch [28/100], loss:0.0099
epoch [29/100], loss:0.0090
epoch [30/100], loss:0.0096
epoch [31/100], loss:0.0082
epoch [32/100], loss:0.0086
epoch [33/100], loss:0.0081
epoch [34/100], loss:0.0077
epoch [35/100], loss:0.0092
epoch [36/100], loss:0.0079
epoch [37/100], loss:0.0073
epoch [38/100], loss:0.0082
epoch [39/100], loss:0.0090
epoch [40/100], loss:0.0077
epoch [41/100], loss:0.0078
epoch [42/100], loss:0.0079
epoch [43/100], loss:0.0078
epoch [44/100], loss:0.0077
epoch [45/100], loss:0.0083
epoch [46/100], loss:0.0080
epoch [47/100], loss:0.0076
epoch [48/100], loss:0.0070
epoch [49/100], loss:0.0077
epoch [50/100], loss:0.0084
epoch [51/100], loss:0.0067
epoch [52/100], loss:0.0075
epoch [53/100], loss:0.0072
epoch [54/100], loss:0.0073
epoch [55/100], loss:0.0069
epoch [56/100], loss:0.0064
epoch [57/100], loss:0.0072
epoch [58/100], loss:0.0063
epoch [59/100], loss:0.0090
epoch [60/100], loss:0.0065
epoch [61/100], loss:0.0068
epoch [62/100], loss:0.0068
epoch [63/100], loss:0.0077
epoch [64/100], loss:0.0065
epoch [65/100], loss:0.0063
epoch [66/100], loss:0.0071
epoch [67/100], loss:0.0066
epoch [68/100], loss:0.0065
epoch [69/100], loss:0.0066
epoch [70/100], loss:0.0061
epoch [71/100], loss:0.0067
epoch [72/100], loss:0.0070
epoch [73/100], loss:0.0068
epoch [74/100], loss:0.0060
epoch [75/100], loss:0.0064
epoch [76/100], loss:0.0069
epoch [77/100], loss:0.0067
epoch [78/100], loss:0.0058
epoch [79/100], loss:0.0070
epoch [80/100], loss:0.0061
epoch [81/100], loss:0.0064
epoch [82/100], loss:0.0069
epoch [83/100], loss:0.0057
epoch [84/100], loss:0.0063
epoch [85/100], loss:0.0061
epoch [86/100], loss:0.0057
epoch [87/100], loss:0.0068
epoch [88/100], loss:0.0055
epoch [89/100], loss:0.0056
epoch [90/100], loss:0.0058
epoch [91/100], loss:0.0056
epoch [92/100], loss:0.0056
epoch [93/100], loss:0.0061
epoch [94/100], loss:0.0070
epoch [95/100], loss:0.0060
epoch [96/100], loss:0.0065
epoch [97/100], loss:0.0063
epoch [98/100], loss:0.0060
epoch [99/100], loss:0.0053
epoch [100/100], loss:0.0066
