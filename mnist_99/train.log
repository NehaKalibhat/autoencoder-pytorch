Logging************pruning 99 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 926 / 1152  0.8038194444444444
encoder.0.bias : 89 / 128  0.6953125
encoder.2.weight : 131048 / 131072  0.99981689453125
encoder.2.bias : 64 / 64  1.0
encoder.4.weight : 7993 / 8192  0.9757080078125
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4410 / 4608  0.95703125
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 1099 / 1152  0.9539930555555556
encoder.9.bias : 8 / 8  1.0
decoder.0.weight : 8906 / 9216  0.9663628472222222
decoder.0.bias : 117 / 128  0.9140625
decoder.2.weight : 32379 / 32768  0.988128662109375
decoder.2.bias : 32 / 64  0.5
decoder.4.weight : 7818 / 8192  0.954345703125
decoder.4.bias : 19 / 32  0.59375
decoder.6.weight : 12621 / 12800  0.986015625
decoder.6.bias : 15 / 16  0.9375
decoder.8.weight : 14 / 64  0.21875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 207607/209705 = 0.9899954698266613
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.3354
epoch [2/100], loss:0.3131
epoch [3/100], loss:0.3015
epoch [4/100], loss:0.3029
epoch [5/100], loss:0.3147
epoch [6/100], loss:0.3141
epoch [7/100], loss:0.3008
epoch [8/100], loss:0.3028
epoch [9/100], loss:0.2961
epoch [10/100], loss:0.3043
epoch [11/100], loss:0.3032
epoch [12/100], loss:0.2946
epoch [13/100], loss:0.2913
epoch [14/100], loss:0.3008
epoch [15/100], loss:0.2951
epoch [16/100], loss:0.3010
epoch [17/100], loss:0.3092
epoch [18/100], loss:0.2925
epoch [19/100], loss:0.3089
epoch [20/100], loss:0.3000
epoch [21/100], loss:0.3129
epoch [22/100], loss:0.2991
epoch [23/100], loss:0.3105
epoch [24/100], loss:0.3066
epoch [25/100], loss:0.3106
epoch [26/100], loss:0.3174
epoch [27/100], loss:0.3007
epoch [28/100], loss:0.3035
epoch [29/100], loss:0.3208
epoch [30/100], loss:0.3026
epoch [31/100], loss:0.2911
epoch [32/100], loss:0.3087
epoch [33/100], loss:0.3200
epoch [34/100], loss:0.3027
epoch [35/100], loss:0.3190
epoch [36/100], loss:0.2995
epoch [37/100], loss:0.3020
epoch [38/100], loss:0.3160
epoch [39/100], loss:0.3240
epoch [40/100], loss:0.2978
epoch [41/100], loss:0.3061
epoch [42/100], loss:0.3183
epoch [43/100], loss:0.3085
epoch [44/100], loss:0.3138
epoch [45/100], loss:0.3093
epoch [46/100], loss:0.3187
epoch [47/100], loss:0.3066
epoch [48/100], loss:0.3101
epoch [49/100], loss:0.2989
epoch [50/100], loss:0.3102
epoch [51/100], loss:0.2874
epoch [52/100], loss:0.3107
epoch [53/100], loss:0.2951
epoch [54/100], loss:0.3191
epoch [55/100], loss:0.3133
epoch [56/100], loss:0.3171
epoch [57/100], loss:0.2996
epoch [58/100], loss:0.2927
epoch [59/100], loss:0.3124
epoch [60/100], loss:0.3101
epoch [61/100], loss:0.3121
epoch [62/100], loss:0.3013
epoch [63/100], loss:0.2949
epoch [64/100], loss:0.3122
epoch [65/100], loss:0.3110
epoch [66/100], loss:0.2998
epoch [67/100], loss:0.3115
epoch [68/100], loss:0.3006
epoch [69/100], loss:0.3233
epoch [70/100], loss:0.3187
epoch [71/100], loss:0.3127
epoch [72/100], loss:0.3126
epoch [73/100], loss:0.3041
epoch [74/100], loss:0.3038
epoch [75/100], loss:0.3085
epoch [76/100], loss:0.3127
epoch [77/100], loss:0.3095
epoch [78/100], loss:0.3005
epoch [79/100], loss:0.3020
epoch [80/100], loss:0.3190
epoch [81/100], loss:0.3018
epoch [82/100], loss:0.3059
epoch [83/100], loss:0.3108
epoch [84/100], loss:0.3167
epoch [85/100], loss:0.2950
epoch [86/100], loss:0.3186
epoch [87/100], loss:0.2981
epoch [88/100], loss:0.3133
epoch [89/100], loss:0.2988
epoch [90/100], loss:0.2946
epoch [91/100], loss:0.2924
epoch [92/100], loss:0.3008
epoch [93/100], loss:0.2889
epoch [94/100], loss:0.3177
epoch [95/100], loss:0.3113
epoch [96/100], loss:0.3054
epoch [97/100], loss:0.3208
epoch [98/100], loss:0.3217
epoch [99/100], loss:0.3175
epoch [100/100], loss:0.3053
