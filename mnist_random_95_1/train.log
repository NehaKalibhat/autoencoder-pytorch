Logging************pruning 95 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 699 / 1152  0.6067708333333334
encoder.0.bias : 72 / 128  0.5625
encoder.2.weight : 130439 / 131072  0.9951705932617188
encoder.2.bias : 64 / 64  1.0
encoder.4.weight : 7548 / 8192  0.92138671875
encoder.4.bias : 30 / 32  0.9375
encoder.6.weight : 3847 / 4608  0.8348524305555556
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 985 / 1152  0.8550347222222222
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8126 / 9216  0.8817274305555556
decoder.0.bias : 101 / 128  0.7890625
decoder.2.weight : 29390 / 32768  0.89691162109375
decoder.2.bias : 20 / 64  0.3125
decoder.4.weight : 6777 / 8192  0.8272705078125
decoder.4.bias : 11 / 32  0.34375
decoder.6.weight : 11065 / 12800  0.864453125
decoder.6.bias : 10 / 16  0.625
decoder.8.weight : 11 / 64  0.171875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 199219/209705 = 0.9499964235473641
Number of parameters in model 209705
epoch [1/100], loss:0.2315
epoch [2/100], loss:0.2400
epoch [3/100], loss:0.2379
epoch [4/100], loss:0.2278
epoch [5/100], loss:0.2162
epoch [6/100], loss:0.2077
epoch [7/100], loss:0.2334
epoch [8/100], loss:0.2231
epoch [9/100], loss:0.2288
epoch [10/100], loss:0.2204
epoch [11/100], loss:0.2288
epoch [12/100], loss:0.2279
epoch [13/100], loss:0.2179
epoch [14/100], loss:0.2174
epoch [15/100], loss:0.2324
epoch [16/100], loss:0.2318
epoch [17/100], loss:0.2205
epoch [18/100], loss:0.2145
epoch [19/100], loss:0.2268
epoch [20/100], loss:0.2259
epoch [21/100], loss:0.2205
epoch [22/100], loss:0.2327
epoch [23/100], loss:0.2302
epoch [24/100], loss:0.2354
epoch [25/100], loss:0.2243
epoch [26/100], loss:0.2249
epoch [27/100], loss:0.2309
epoch [28/100], loss:0.2206
epoch [29/100], loss:0.2130
epoch [30/100], loss:0.2293
