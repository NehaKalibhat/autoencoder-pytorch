Logging************pruning 91 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 565 / 1152  0.4904513888888889
encoder.0.bias : 63 / 128  0.4921875
encoder.2.weight : 128809 / 131072  0.9827346801757812
encoder.2.bias : 63 / 64  0.984375
encoder.4.weight : 7291 / 8192  0.8900146484375
encoder.4.bias : 27 / 32  0.84375
encoder.6.weight : 3436 / 4608  0.7456597222222222
encoder.6.bias : 13 / 16  0.8125
encoder.9.weight : 860 / 1152  0.7465277777777778
encoder.9.bias : 5 / 8  0.625
decoder.0.weight : 7493 / 9216  0.8130425347222222
decoder.0.bias : 85 / 128  0.6640625
decoder.2.weight : 26414 / 32768  0.80609130859375
decoder.2.bias : 15 / 64  0.234375
decoder.4.weight : 6156 / 8192  0.75146484375
decoder.4.bias : 10 / 32  0.3125
decoder.6.weight : 9506 / 12800  0.74265625
decoder.6.bias : 10 / 16  0.625
decoder.8.weight : 9 / 64  0.140625
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 190831/209705 = 0.9099973772680671
Number of parameters in model 209705
epoch [1/100], loss:0.1513
epoch [2/100], loss:0.0950
epoch [3/100], loss:0.0914
epoch [4/100], loss:0.0778
epoch [5/100], loss:0.0726
epoch [6/100], loss:0.0774
epoch [7/100], loss:0.0701
epoch [8/100], loss:0.0722
epoch [9/100], loss:0.0663
epoch [10/100], loss:0.0653
epoch [11/100], loss:0.0605
epoch [12/100], loss:0.0551
epoch [13/100], loss:0.0529
epoch [14/100], loss:0.0525
epoch [15/100], loss:0.0528
epoch [16/100], loss:0.0504
epoch [17/100], loss:0.0541
epoch [18/100], loss:0.0505
epoch [19/100], loss:0.0487
epoch [20/100], loss:0.0485
epoch [21/100], loss:0.0497
epoch [22/100], loss:0.0461
epoch [23/100], loss:0.0401
epoch [24/100], loss:0.0459
epoch [25/100], loss:0.0465
epoch [26/100], loss:0.0448
epoch [27/100], loss:0.0445
epoch [28/100], loss:0.0409
epoch [29/100], loss:0.0447
epoch [30/100], loss:0.0427
epoch [31/100], loss:0.0418
epoch [32/100], loss:0.0394
epoch [33/100], loss:0.0429
epoch [34/100], loss:0.0430
epoch [35/100], loss:0.0455
epoch [36/100], loss:0.0382
epoch [37/100], loss:0.0413
epoch [38/100], loss:0.0395
epoch [39/100], loss:0.0383
epoch [40/100], loss:0.0403
epoch [41/100], loss:0.0378
epoch [42/100], loss:0.0381
epoch [43/100], loss:0.0393
epoch [44/100], loss:0.0367
epoch [45/100], loss:0.0421
epoch [46/100], loss:0.0394
epoch [47/100], loss:0.0395
epoch [48/100], loss:0.0414
epoch [49/100], loss:0.0372
epoch [50/100], loss:0.0386
epoch [51/100], loss:0.0331
epoch [52/100], loss:0.0344
epoch [53/100], loss:0.0385
epoch [54/100], loss:0.0358
epoch [55/100], loss:0.0348
epoch [56/100], loss:0.0333
epoch [57/100], loss:0.0386
epoch [58/100], loss:0.0319
epoch [59/100], loss:0.0325
epoch [60/100], loss:0.0339
epoch [61/100], loss:0.0359
epoch [62/100], loss:0.0331
epoch [63/100], loss:0.0326
epoch [64/100], loss:0.0355
epoch [65/100], loss:0.0368
epoch [66/100], loss:0.0332
epoch [67/100], loss:0.0313
epoch [68/100], loss:0.0356
epoch [69/100], loss:0.0335
epoch [70/100], loss:0.0339
epoch [71/100], loss:0.0325
epoch [72/100], loss:0.0367
epoch [73/100], loss:0.0335
epoch [74/100], loss:0.0333
epoch [75/100], loss:0.0327
epoch [76/100], loss:0.0333
epoch [77/100], loss:0.0338
epoch [78/100], loss:0.0277
epoch [79/100], loss:0.0315
epoch [80/100], loss:0.0316
epoch [81/100], loss:0.0295
epoch [82/100], loss:0.0300
epoch [83/100], loss:0.0321
epoch [84/100], loss:0.0287
epoch [85/100], loss:0.0293
epoch [86/100], loss:0.0297
epoch [87/100], loss:0.0301
epoch [88/100], loss:0.0297
epoch [89/100], loss:0.0323
epoch [90/100], loss:0.0319
epoch [91/100], loss:0.0304
epoch [92/100], loss:0.0280
epoch [93/100], loss:0.0292
epoch [94/100], loss:0.0303
epoch [95/100], loss:0.0295
epoch [96/100], loss:0.0324
epoch [97/100], loss:0.0304
epoch [98/100], loss:0.0281
epoch [99/100], loss:0.0315
epoch [100/100], loss:0.0296
