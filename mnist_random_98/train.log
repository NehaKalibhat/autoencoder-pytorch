Logging************pruning 98 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 844 / 1152  0.7326388888888888
encoder.0.bias : 79 / 128  0.6171875
encoder.2.weight : 130977 / 131072  0.9992752075195312
encoder.2.bias : 64 / 64  1.0
encoder.4.weight : 7860 / 8192  0.95947265625
encoder.4.bias : 32 / 32  1.0
encoder.6.weight : 4257 / 4608  0.923828125
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 1063 / 1152  0.9227430555555556
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8651 / 9216  0.9386935763888888
decoder.0.bias : 110 / 128  0.859375
decoder.2.weight : 31685 / 32768  0.966949462890625
decoder.2.bias : 29 / 64  0.453125
decoder.4.weight : 7488 / 8192  0.9140625
decoder.4.bias : 16 / 32  0.5
decoder.6.weight : 12306 / 12800  0.96140625
decoder.6.bias : 12 / 16  0.75
decoder.8.weight : 13 / 64  0.203125
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 205510/209705 = 0.979995708256837
Number of parameters in model 209705
epoch [1/100], loss:0.2954
epoch [2/100], loss:0.2279
epoch [3/100], loss:0.2390
epoch [4/100], loss:0.2376
epoch [5/100], loss:0.2300
epoch [6/100], loss:0.2377
epoch [7/100], loss:0.2385
epoch [8/100], loss:0.2335
epoch [9/100], loss:0.2299
epoch [10/100], loss:0.2295
epoch [11/100], loss:0.2325
epoch [12/100], loss:0.2287
epoch [13/100], loss:0.2328
epoch [14/100], loss:0.2180
epoch [15/100], loss:0.2280
epoch [16/100], loss:0.2227
epoch [17/100], loss:0.2189
epoch [18/100], loss:0.2280
epoch [19/100], loss:0.2275
epoch [20/100], loss:0.2184
epoch [21/100], loss:0.2338
epoch [22/100], loss:0.2290
epoch [23/100], loss:0.2185
epoch [24/100], loss:0.2175
epoch [25/100], loss:0.2276
epoch [26/100], loss:0.2168
epoch [27/100], loss:0.2251
epoch [28/100], loss:0.2407
epoch [29/100], loss:0.2464
epoch [30/100], loss:0.2278
epoch [31/100], loss:0.2272
epoch [32/100], loss:0.2287
epoch [33/100], loss:0.2188
epoch [34/100], loss:0.2324
epoch [35/100], loss:0.2331
epoch [36/100], loss:0.2141
epoch [37/100], loss:0.2320
epoch [38/100], loss:0.2262
epoch [39/100], loss:0.2304
epoch [40/100], loss:0.2221
epoch [41/100], loss:0.2170
epoch [42/100], loss:0.2361
epoch [43/100], loss:0.2300
epoch [44/100], loss:0.2181
epoch [45/100], loss:0.2160
epoch [46/100], loss:0.2227
epoch [47/100], loss:0.2411
epoch [48/100], loss:0.2283
epoch [49/100], loss:0.2143
epoch [50/100], loss:0.2288
epoch [51/100], loss:0.2301
epoch [52/100], loss:0.2183
epoch [53/100], loss:0.2216
epoch [54/100], loss:0.2200
epoch [55/100], loss:0.2339
epoch [56/100], loss:0.2233
epoch [57/100], loss:0.2265
epoch [58/100], loss:0.2286
epoch [59/100], loss:0.2314
epoch [60/100], loss:0.2355
epoch [61/100], loss:0.2251
epoch [62/100], loss:0.2233
epoch [63/100], loss:0.2268
epoch [64/100], loss:0.2375
epoch [65/100], loss:0.2424
epoch [66/100], loss:0.2351
epoch [67/100], loss:0.2413
epoch [68/100], loss:0.2296
epoch [69/100], loss:0.2285
epoch [70/100], loss:0.2268
epoch [71/100], loss:0.2249
epoch [72/100], loss:0.2158
epoch [73/100], loss:0.2299
epoch [74/100], loss:0.2257
epoch [75/100], loss:0.2268
epoch [76/100], loss:0.2054
epoch [77/100], loss:0.2376
epoch [78/100], loss:0.2308
epoch [79/100], loss:0.2216
epoch [80/100], loss:0.2277
epoch [81/100], loss:0.2261
epoch [82/100], loss:0.2210
epoch [83/100], loss:0.2212
epoch [84/100], loss:0.2194
epoch [85/100], loss:0.2249
epoch [86/100], loss:0.2413
epoch [87/100], loss:0.2277
epoch [88/100], loss:0.2331
epoch [89/100], loss:0.2265
epoch [90/100], loss:0.2289
epoch [91/100], loss:0.2172
epoch [92/100], loss:0.2347
epoch [93/100], loss:0.2226
epoch [94/100], loss:0.2341
epoch [95/100], loss:0.2254
epoch [96/100], loss:0.2314
epoch [97/100], loss:0.2189
epoch [98/100], loss:0.2273
epoch [99/100], loss:0.2247
epoch [100/100], loss:0.2270
