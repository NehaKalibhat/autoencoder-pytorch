Logging************pruning 92 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 595 / 1152  0.5164930555555556
encoder.0.bias : 64 / 128  0.5
encoder.2.weight : 129327 / 131072  0.9866867065429688
encoder.2.bias : 63 / 64  0.984375
encoder.4.weight : 7347 / 8192  0.8968505859375
encoder.4.bias : 28 / 32  0.875
encoder.6.weight : 3537 / 4608  0.767578125
encoder.6.bias : 14 / 16  0.875
encoder.9.weight : 896 / 1152  0.7777777777777778
encoder.9.bias : 5 / 8  0.625
decoder.0.weight : 7631 / 9216  0.8280164930555556
decoder.0.bias : 91 / 128  0.7109375
decoder.2.weight : 27121 / 32768  0.827667236328125
decoder.2.bias : 15 / 64  0.234375
decoder.4.weight : 6295 / 8192  0.7684326171875
decoder.4.bias : 10 / 32  0.3125
decoder.6.weight : 9869 / 12800  0.771015625
decoder.6.bias : 10 / 16  0.625
decoder.8.weight : 9 / 64  0.140625
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 192928/209705 = 0.9199971388378914
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1606
epoch [2/100], loss:0.1081
epoch [3/100], loss:0.0964
epoch [4/100], loss:0.0985
epoch [5/100], loss:0.0849
epoch [6/100], loss:0.0795
epoch [7/100], loss:0.0717
epoch [8/100], loss:0.0783
epoch [9/100], loss:0.0728
epoch [10/100], loss:0.0784
epoch [11/100], loss:0.0735
epoch [12/100], loss:0.0690
epoch [13/100], loss:0.0628
epoch [14/100], loss:0.0592
epoch [15/100], loss:0.0601
epoch [16/100], loss:0.0649
epoch [17/100], loss:0.0631
epoch [18/100], loss:0.0605
epoch [19/100], loss:0.0613
epoch [20/100], loss:0.0659
epoch [21/100], loss:0.0661
epoch [22/100], loss:0.0599
epoch [23/100], loss:0.0595
epoch [24/100], loss:0.0608
epoch [25/100], loss:0.0596
epoch [26/100], loss:0.0581
epoch [27/100], loss:0.0596
epoch [28/100], loss:0.0568
epoch [29/100], loss:0.0572
epoch [30/100], loss:0.0593
epoch [31/100], loss:0.0612
epoch [32/100], loss:0.0542
epoch [33/100], loss:0.0520
epoch [34/100], loss:0.0477
epoch [35/100], loss:0.0547
epoch [36/100], loss:0.0497
epoch [37/100], loss:0.0507
epoch [38/100], loss:0.0519
epoch [39/100], loss:0.0531
epoch [40/100], loss:0.0583
epoch [41/100], loss:0.0478
epoch [42/100], loss:0.0476
epoch [43/100], loss:0.0533
epoch [44/100], loss:0.0491
epoch [45/100], loss:0.0503
epoch [46/100], loss:0.0496
epoch [47/100], loss:0.0484
epoch [48/100], loss:0.0559
epoch [49/100], loss:0.0534
epoch [50/100], loss:0.0458
epoch [51/100], loss:0.0451
epoch [52/100], loss:0.0498
epoch [53/100], loss:0.0520
epoch [54/100], loss:0.0467
epoch [55/100], loss:0.0513
epoch [56/100], loss:0.0506
epoch [57/100], loss:0.0487
epoch [58/100], loss:0.0447
epoch [59/100], loss:0.0450
epoch [60/100], loss:0.0524
epoch [61/100], loss:0.0444
epoch [62/100], loss:0.0474
epoch [63/100], loss:0.0474
epoch [64/100], loss:0.0438
epoch [65/100], loss:0.0453
epoch [66/100], loss:0.0476
epoch [67/100], loss:0.0522
epoch [68/100], loss:0.0407
epoch [69/100], loss:0.0454
epoch [70/100], loss:0.0440
epoch [71/100], loss:0.0433
epoch [72/100], loss:0.0482
epoch [73/100], loss:0.0430
epoch [74/100], loss:0.0427
epoch [75/100], loss:0.0426
epoch [76/100], loss:0.0448
epoch [77/100], loss:0.0466
epoch [78/100], loss:0.0409
epoch [79/100], loss:0.0449
epoch [80/100], loss:0.0443
epoch [81/100], loss:0.0417
epoch [82/100], loss:0.0438
epoch [83/100], loss:0.0459
epoch [84/100], loss:0.0404
epoch [85/100], loss:0.0380
epoch [86/100], loss:0.0392
epoch [87/100], loss:0.0449
epoch [88/100], loss:0.0469
epoch [89/100], loss:0.0424
epoch [90/100], loss:0.0408
epoch [91/100], loss:0.0482
epoch [92/100], loss:0.0445
epoch [93/100], loss:0.0415
epoch [94/100], loss:0.0466
epoch [95/100], loss:0.0422
epoch [96/100], loss:0.0426
epoch [97/100], loss:0.0441
epoch [98/100], loss:0.0413
epoch [99/100], loss:0.0458
epoch [100/100], loss:0.0328
