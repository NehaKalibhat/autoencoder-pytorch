Logging************pruning 94 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 663 / 1152  0.5755208333333334
encoder.0.bias : 70 / 128  0.546875
encoder.2.weight : 130138 / 131072  0.9928741455078125
encoder.2.bias : 63 / 64  0.984375
encoder.4.weight : 7463 / 8192  0.9110107421875
encoder.4.bias : 30 / 32  0.9375
encoder.6.weight : 3731 / 4608  0.8096788194444444
encoder.6.bias : 15 / 16  0.9375
encoder.9.weight : 958 / 1152  0.8315972222222222
encoder.9.bias : 6 / 8  0.75
decoder.0.weight : 7950 / 9216  0.8626302083333334
decoder.0.bias : 94 / 128  0.734375
decoder.2.weight : 28631 / 32768  0.873748779296875
decoder.2.bias : 18 / 64  0.28125
decoder.4.weight : 6623 / 8192  0.8084716796875
decoder.4.bias : 10 / 32  0.3125
decoder.6.weight : 10638 / 12800  0.83109375
decoder.6.bias : 10 / 16  0.625
decoder.8.weight : 10 / 64  0.15625
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 197122/209705 = 0.9399966619775398
Number of parameters in model 209705
epoch [1/100], loss:0.1686
epoch [2/100], loss:0.1247
epoch [3/100], loss:0.1012
epoch [4/100], loss:0.0942
epoch [5/100], loss:0.0889
epoch [6/100], loss:0.0880
epoch [7/100], loss:0.0839
epoch [8/100], loss:0.0771
epoch [9/100], loss:0.0738
epoch [10/100], loss:0.0677
epoch [11/100], loss:0.0669
epoch [12/100], loss:0.0713
epoch [13/100], loss:0.0602
epoch [14/100], loss:0.0652
epoch [15/100], loss:0.0670
epoch [16/100], loss:0.0588
epoch [17/100], loss:0.0628
epoch [18/100], loss:0.0612
epoch [19/100], loss:0.0557
epoch [20/100], loss:0.0641
epoch [21/100], loss:0.0631
epoch [22/100], loss:0.0675
epoch [23/100], loss:0.0613
epoch [24/100], loss:0.0573
epoch [25/100], loss:0.0566
epoch [26/100], loss:0.0564
epoch [27/100], loss:0.0554
epoch [28/100], loss:0.0563
epoch [29/100], loss:0.0650
epoch [30/100], loss:0.0538
epoch [31/100], loss:0.0618
epoch [32/100], loss:0.0463
epoch [33/100], loss:0.0555
epoch [34/100], loss:0.0556
epoch [35/100], loss:0.0566
epoch [36/100], loss:0.0543
epoch [37/100], loss:0.0561
epoch [38/100], loss:0.0474
epoch [39/100], loss:0.0522
epoch [40/100], loss:0.0527
epoch [41/100], loss:0.0573
epoch [42/100], loss:0.0516
epoch [43/100], loss:0.0477
epoch [44/100], loss:0.0492
epoch [45/100], loss:0.0529
epoch [46/100], loss:0.0550
epoch [47/100], loss:0.0522
epoch [48/100], loss:0.0603
epoch [49/100], loss:0.0517
epoch [50/100], loss:0.0509
epoch [51/100], loss:0.0473
epoch [52/100], loss:0.0529
epoch [53/100], loss:0.0580
epoch [54/100], loss:0.0503
epoch [55/100], loss:0.0471
epoch [56/100], loss:0.0465
epoch [57/100], loss:0.0498
epoch [58/100], loss:0.0436
epoch [59/100], loss:0.0508
epoch [60/100], loss:0.0500
epoch [61/100], loss:0.0478
epoch [62/100], loss:0.0498
epoch [63/100], loss:0.0490
epoch [64/100], loss:0.0520
epoch [65/100], loss:0.0538
epoch [66/100], loss:0.0466
epoch [67/100], loss:0.0504
epoch [68/100], loss:0.0486
epoch [69/100], loss:0.0467
epoch [70/100], loss:0.0462
epoch [71/100], loss:0.0501
epoch [72/100], loss:0.0482
epoch [73/100], loss:0.0480
epoch [74/100], loss:0.0518
epoch [75/100], loss:0.0474
epoch [76/100], loss:0.0496
epoch [77/100], loss:0.0556
epoch [78/100], loss:0.0494
epoch [79/100], loss:0.0477
epoch [80/100], loss:0.0488
epoch [81/100], loss:0.0465
epoch [82/100], loss:0.0444
epoch [83/100], loss:0.0450
epoch [84/100], loss:0.0494
epoch [85/100], loss:0.0461
epoch [86/100], loss:0.0440
epoch [87/100], loss:0.0542
epoch [88/100], loss:0.0458
epoch [89/100], loss:0.0535
epoch [90/100], loss:0.0535
epoch [91/100], loss:0.0540
epoch [92/100], loss:0.0513
epoch [93/100], loss:0.0488
epoch [94/100], loss:0.0430
epoch [95/100], loss:0.0495
epoch [96/100], loss:0.0448
epoch [97/100], loss:0.0455
epoch [98/100], loss:0.0436
epoch [99/100], loss:0.0470
epoch [100/100], loss:0.0451
