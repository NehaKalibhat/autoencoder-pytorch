Logging************pruning 95 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 699 / 1152  0.6067708333333334
encoder.0.bias : 72 / 128  0.5625
encoder.2.weight : 130439 / 131072  0.9951705932617188
encoder.2.bias : 64 / 64  1.0
encoder.4.weight : 7548 / 8192  0.92138671875
encoder.4.bias : 30 / 32  0.9375
encoder.6.weight : 3847 / 4608  0.8348524305555556
encoder.6.bias : 16 / 16  1.0
encoder.9.weight : 985 / 1152  0.8550347222222222
encoder.9.bias : 7 / 8  0.875
decoder.0.weight : 8126 / 9216  0.8817274305555556
decoder.0.bias : 101 / 128  0.7890625
decoder.2.weight : 29390 / 32768  0.89691162109375
decoder.2.bias : 20 / 64  0.3125
decoder.4.weight : 6777 / 8192  0.8272705078125
decoder.4.bias : 11 / 32  0.34375
decoder.6.weight : 11065 / 12800  0.864453125
decoder.6.bias : 10 / 16  0.625
decoder.8.weight : 11 / 64  0.171875
decoder.8.bias : 1 / 1  1.0
Fraction of weights pruned = 199219/209705 = 0.9499964235473641
Number of parameters in model 209705
Loading saved autoencoder named ./mnist/before_train.pth
epoch [1/100], loss:0.1744
epoch [2/100], loss:0.1335
epoch [3/100], loss:0.1217
epoch [4/100], loss:0.1114
epoch [5/100], loss:0.1111
epoch [6/100], loss:0.1041
epoch [7/100], loss:0.1020
epoch [8/100], loss:0.0963
epoch [9/100], loss:0.0952
epoch [10/100], loss:0.0875
epoch [11/100], loss:0.0931
epoch [12/100], loss:0.0797
epoch [13/100], loss:0.0863
epoch [14/100], loss:0.0810
epoch [15/100], loss:0.0713
epoch [16/100], loss:0.0724
epoch [17/100], loss:0.0757
epoch [18/100], loss:0.0724
epoch [19/100], loss:0.0697
epoch [20/100], loss:0.0733
epoch [21/100], loss:0.0734
epoch [22/100], loss:0.0677
epoch [23/100], loss:0.0661
epoch [24/100], loss:0.0730
epoch [25/100], loss:0.0682
epoch [26/100], loss:0.0704
epoch [27/100], loss:0.0585
epoch [28/100], loss:0.0644
epoch [29/100], loss:0.0650
epoch [30/100], loss:0.0673
epoch [31/100], loss:0.0650
epoch [32/100], loss:0.0690
epoch [33/100], loss:0.0646
epoch [34/100], loss:0.0579
epoch [35/100], loss:0.0567
epoch [36/100], loss:0.0585
epoch [37/100], loss:0.0578
epoch [38/100], loss:0.0621
epoch [39/100], loss:0.0590
epoch [40/100], loss:0.0542
epoch [41/100], loss:0.0560
epoch [42/100], loss:0.0564
epoch [43/100], loss:0.0558
epoch [44/100], loss:0.0568
epoch [45/100], loss:0.0557
epoch [46/100], loss:0.0488
epoch [47/100], loss:0.0596
epoch [48/100], loss:0.0549
epoch [49/100], loss:0.0594
epoch [50/100], loss:0.0558
epoch [51/100], loss:0.0562
epoch [52/100], loss:0.0569
epoch [53/100], loss:0.0571
epoch [54/100], loss:0.0501
epoch [55/100], loss:0.0490
epoch [56/100], loss:0.0537
epoch [57/100], loss:0.0502
epoch [58/100], loss:0.0583
epoch [59/100], loss:0.0537
epoch [60/100], loss:0.0580
epoch [61/100], loss:0.0506
epoch [62/100], loss:0.0551
epoch [63/100], loss:0.0548
epoch [64/100], loss:0.0496
epoch [65/100], loss:0.0508
epoch [66/100], loss:0.0467
epoch [67/100], loss:0.0483
epoch [68/100], loss:0.0485
epoch [69/100], loss:0.0520
epoch [70/100], loss:0.0544
epoch [71/100], loss:0.0480
epoch [72/100], loss:0.0504
epoch [73/100], loss:0.0456
epoch [74/100], loss:0.0545
epoch [75/100], loss:0.0478
epoch [76/100], loss:0.0508
epoch [77/100], loss:0.0481
epoch [78/100], loss:0.0506
epoch [79/100], loss:0.0486
epoch [80/100], loss:0.0478
epoch [81/100], loss:0.0496
epoch [82/100], loss:0.0493
epoch [83/100], loss:0.0468
epoch [84/100], loss:0.0440
epoch [85/100], loss:0.0411
epoch [86/100], loss:0.0418
epoch [87/100], loss:0.0529
epoch [88/100], loss:0.0450
epoch [89/100], loss:0.0456
epoch [90/100], loss:0.0468
epoch [91/100], loss:0.0462
epoch [92/100], loss:0.0469
epoch [93/100], loss:0.0492
epoch [94/100], loss:0.0520
epoch [95/100], loss:0.0442
epoch [96/100], loss:0.0473
epoch [97/100], loss:0.0508
epoch [98/100], loss:0.0480
epoch [99/100], loss:0.0478
epoch [100/100], loss:0.0508
