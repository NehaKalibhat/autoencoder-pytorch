************pruning 95 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 1412 / 2304  0.6128472222222222
encoder.0.bias : 136 / 256  0.53125
encoder.2.weight : 519151 / 524288  0.9902019500732422
encoder.2.bias : 123 / 128  0.9609375
encoder.4.weight : 31081 / 32768  0.948516845703125
encoder.4.bias : 59 / 64  0.921875
encoder.6.weight : 14559 / 18432  0.7898763020833334
encoder.6.bias : 31 / 32  0.96875
encoder.9.weight : 3769 / 4608  0.8179253472222222
encoder.9.bias : 12 / 16  0.75
decoder.0.weight : 33760 / 36864  0.9157986111111112
decoder.0.bias : 223 / 256  0.87109375
decoder.2.weight : 119402 / 131072  0.9109649658203125
decoder.2.bias : 83 / 128  0.6484375
decoder.4.weight : 26439 / 32768  0.806854248046875
decoder.4.bias : 24 / 64  0.375
decoder.6.weight : 43305 / 51200  0.84580078125
decoder.6.bias : 14 / 32  0.4375
decoder.8.weight : 55 / 128  0.4296875
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 793638/835409 = 0.9499993416398435
Number of parameters in model 835409
Loading saved autoencoder named ./mnist/before_train.pth
tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,
        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,
        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,
        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,
        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,
        2, 0, 2, 7, 1, 8, 6, 4])
epoch [1/100], loss:0.4287
epoch [2/100], loss:0.4243
epoch [3/100], loss:0.4147
epoch [4/100], loss:0.1565
epoch [5/100], loss:0.0815
epoch [6/100], loss:0.0721
epoch [7/100], loss:0.0586
epoch [8/100], loss:0.0675
epoch [9/100], loss:0.0534
epoch [10/100], loss:0.0511
epoch [11/100], loss:0.0492
epoch [12/100], loss:0.0431
epoch [13/100], loss:0.0459
epoch [14/100], loss:0.0413
epoch [15/100], loss:0.0456
epoch [16/100], loss:0.0457
epoch [17/100], loss:0.0414
epoch [18/100], loss:0.0406
epoch [19/100], loss:0.0367
epoch [20/100], loss:0.0362
epoch [21/100], loss:0.0377
epoch [22/100], loss:0.0348
epoch [23/100], loss:0.0411
epoch [24/100], loss:0.0362
epoch [25/100], loss:0.0373
epoch [26/100], loss:0.0330
epoch [27/100], loss:0.0334
epoch [28/100], loss:0.0347
epoch [29/100], loss:0.0300
epoch [30/100], loss:0.0318
epoch [31/100], loss:0.0344
epoch [32/100], loss:0.0315
epoch [33/100], loss:0.0348
epoch [34/100], loss:0.0304
epoch [35/100], loss:0.0312
epoch [36/100], loss:0.0270
epoch [37/100], loss:0.0287
epoch [38/100], loss:0.0298
epoch [39/100], loss:0.0297
epoch [40/100], loss:0.0305
epoch [41/100], loss:0.0317
epoch [42/100], loss:0.0310
epoch [43/100], loss:0.0282
epoch [44/100], loss:0.0288
epoch [45/100], loss:0.0297
epoch [46/100], loss:0.0278
epoch [47/100], loss:0.0277
epoch [48/100], loss:0.0278
epoch [49/100], loss:0.0275
epoch [50/100], loss:0.0267
epoch [51/100], loss:0.0278
epoch [52/100], loss:0.0270
epoch [53/100], loss:0.0278
epoch [54/100], loss:0.0290
epoch [55/100], loss:0.0249
epoch [56/100], loss:0.0247
epoch [57/100], loss:0.0253
epoch [58/100], loss:0.0291
epoch [59/100], loss:0.0252
epoch [60/100], loss:0.0273
epoch [61/100], loss:0.0245
epoch [62/100], loss:0.0273
epoch [63/100], loss:0.0273
epoch [64/100], loss:0.0251
epoch [65/100], loss:0.0257
epoch [66/100], loss:0.0249
epoch [67/100], loss:0.0256
epoch [68/100], loss:0.0277
epoch [69/100], loss:0.0255
epoch [70/100], loss:0.0250
epoch [71/100], loss:0.0255
epoch [72/100], loss:0.0259
epoch [73/100], loss:0.0249
epoch [74/100], loss:0.0235
epoch [75/100], loss:0.0239
epoch [76/100], loss:0.0252
epoch [77/100], loss:0.0268
epoch [78/100], loss:0.0237
epoch [79/100], loss:0.0251
epoch [80/100], loss:0.0256
epoch [81/100], loss:0.0246
epoch [82/100], loss:0.0285
epoch [83/100], loss:0.0272
epoch [84/100], loss:0.0222
epoch [85/100], loss:0.0237
epoch [86/100], loss:0.0267
epoch [87/100], loss:0.0264
epoch [88/100], loss:0.0249
epoch [89/100], loss:0.0267
epoch [90/100], loss:0.0239
epoch [91/100], loss:0.0272
epoch [92/100], loss:0.0220
epoch [93/100], loss:0.0241
epoch [94/100], loss:0.0238
epoch [95/100], loss:0.0224
epoch [96/100], loss:0.0250
epoch [97/100], loss:0.0239
epoch [98/100], loss:0.0208
epoch [99/100], loss:0.0268
epoch [100/100], loss:0.0241
