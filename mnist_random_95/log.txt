************pruning 95 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 1412 / 2304  0.6128472222222222
encoder.0.bias : 136 / 256  0.53125
encoder.2.weight : 519151 / 524288  0.9902019500732422
encoder.2.bias : 123 / 128  0.9609375
encoder.4.weight : 31081 / 32768  0.948516845703125
encoder.4.bias : 59 / 64  0.921875
encoder.6.weight : 14559 / 18432  0.7898763020833334
encoder.6.bias : 31 / 32  0.96875
encoder.9.weight : 3769 / 4608  0.8179253472222222
encoder.9.bias : 12 / 16  0.75
decoder.0.weight : 33760 / 36864  0.9157986111111112
decoder.0.bias : 223 / 256  0.87109375
decoder.2.weight : 119402 / 131072  0.9109649658203125
decoder.2.bias : 83 / 128  0.6484375
decoder.4.weight : 26439 / 32768  0.806854248046875
decoder.4.bias : 24 / 64  0.375
decoder.6.weight : 43305 / 51200  0.84580078125
decoder.6.bias : 14 / 32  0.4375
decoder.8.weight : 55 / 128  0.4296875
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 793638/835409 = 0.9499993416398435
Number of parameters in model 835409
tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,
        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,
        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,
        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,
        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,
        2, 0, 2, 7, 1, 8, 6, 4])
epoch [1/100], loss:0.1496
epoch [2/100], loss:0.0914
epoch [3/100], loss:0.0793
epoch [4/100], loss:0.0736
epoch [5/100], loss:0.0702
epoch [6/100], loss:0.0587
epoch [7/100], loss:0.0636
epoch [8/100], loss:0.0522
epoch [9/100], loss:0.0537
epoch [10/100], loss:0.0544
epoch [11/100], loss:0.0503
epoch [12/100], loss:0.0532
epoch [13/100], loss:0.0516
epoch [14/100], loss:0.0465
epoch [15/100], loss:0.0488
epoch [16/100], loss:0.0478
epoch [17/100], loss:0.0451
epoch [18/100], loss:0.0451
epoch [19/100], loss:0.0461
epoch [20/100], loss:0.0429
epoch [21/100], loss:0.0409
epoch [22/100], loss:0.0418
epoch [23/100], loss:0.0431
epoch [24/100], loss:0.0391
epoch [25/100], loss:0.0409
epoch [26/100], loss:0.0407
epoch [27/100], loss:0.0379
epoch [28/100], loss:0.0369
epoch [29/100], loss:0.0365
epoch [30/100], loss:0.0393
epoch [31/100], loss:0.0368
epoch [32/100], loss:0.0351
epoch [33/100], loss:0.0347
epoch [34/100], loss:0.0348
epoch [35/100], loss:0.0350
epoch [36/100], loss:0.0318
epoch [37/100], loss:0.0341
epoch [38/100], loss:0.0314
epoch [39/100], loss:0.0344
epoch [40/100], loss:0.0321
epoch [41/100], loss:0.0328
epoch [42/100], loss:0.0324
epoch [43/100], loss:0.0356
epoch [44/100], loss:0.0357
epoch [45/100], loss:0.0332
epoch [46/100], loss:0.0303
epoch [47/100], loss:0.0331
epoch [48/100], loss:0.0325
epoch [49/100], loss:0.0312
epoch [50/100], loss:0.0319
epoch [51/100], loss:0.0285
epoch [52/100], loss:0.0317
epoch [53/100], loss:0.0310
epoch [54/100], loss:0.0354
epoch [55/100], loss:0.0304
epoch [56/100], loss:0.0267
epoch [57/100], loss:0.0249
epoch [58/100], loss:0.0321
epoch [59/100], loss:0.0275
epoch [60/100], loss:0.0316
epoch [61/100], loss:0.0355
epoch [62/100], loss:0.0294
epoch [63/100], loss:0.0317
epoch [64/100], loss:0.0292
epoch [65/100], loss:0.0268
epoch [66/100], loss:0.0295
epoch [67/100], loss:0.0280
epoch [68/100], loss:0.0283
epoch [69/100], loss:0.0301
epoch [70/100], loss:0.0286
epoch [71/100], loss:0.0265
epoch [72/100], loss:0.0284
epoch [73/100], loss:0.0265
epoch [74/100], loss:0.0319
epoch [75/100], loss:0.0246
epoch [76/100], loss:0.0276
epoch [77/100], loss:0.0265
epoch [78/100], loss:0.0293
epoch [79/100], loss:0.0309
epoch [80/100], loss:0.0273
epoch [81/100], loss:0.0305
epoch [82/100], loss:0.0284
epoch [83/100], loss:0.0288
epoch [84/100], loss:0.0273
epoch [85/100], loss:0.0268
epoch [86/100], loss:0.0264
epoch [87/100], loss:0.0280
epoch [88/100], loss:0.0245
epoch [89/100], loss:0.0272
epoch [90/100], loss:0.0284
epoch [91/100], loss:0.0291
epoch [92/100], loss:0.0300
epoch [93/100], loss:0.0253
epoch [94/100], loss:0.0260
epoch [95/100], loss:0.0285
epoch [96/100], loss:0.0293
epoch [97/100], loss:0.0265
epoch [98/100], loss:0.0257
epoch [99/100], loss:0.0276
epoch [100/100], loss:0.0289
