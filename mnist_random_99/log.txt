************pruning 99 of the network****************
Layer-wise pruning = False
Autoencoder layer-wise pruning percentages
encoder.0.weight : 3258 / 4608  0.70703125
encoder.0.bias : 336 / 512  0.65625
encoder.2.weight : 2093902 / 2097152  0.9984502792358398
encoder.2.bias : 246 / 256  0.9609375
encoder.4.weight : 130037 / 131072  0.9921035766601562
encoder.4.bias : 127 / 128  0.9921875
encoder.6.weight : 70528 / 73728  0.9565972222222222
encoder.6.bias : 61 / 64  0.953125
encoder.9.weight : 15396 / 18432  0.8352864583333334
encoder.9.bias : 17 / 32  0.53125
decoder.0.weight : 139854 / 147456  0.9484456380208334
decoder.0.bias : 497 / 512  0.970703125
decoder.2.weight : 518869 / 524288  0.9896640777587891
decoder.2.bias : 238 / 256  0.9296875
decoder.4.weight : 127728 / 131072  0.9744873046875
decoder.4.bias : 77 / 128  0.6015625
decoder.6.weight : 200167 / 204800  0.9773779296875
decoder.6.bias : 27 / 64  0.421875
decoder.8.weight : 103 / 256  0.40234375
decoder.8.bias : 0 / 1  0.0
Fraction of weights pruned = 3301468/3334817 = 0.9899997511107806
Number of parameters in model 3334817
tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,
        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,
        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,
        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,
        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,
        2, 0, 2, 7, 1, 8, 6, 4])
epoch [1/100], loss:0.4159
epoch [2/100], loss:0.4389
epoch [3/100], loss:0.4103
epoch [4/100], loss:0.4331
epoch [5/100], loss:0.1347
epoch [6/100], loss:0.0783
epoch [7/100], loss:0.0677
epoch [8/100], loss:0.0577
epoch [9/100], loss:0.0559
epoch [10/100], loss:0.0503
epoch [11/100], loss:0.0492
epoch [12/100], loss:0.0487
epoch [13/100], loss:0.0485
epoch [14/100], loss:0.0408
epoch [15/100], loss:0.0410
epoch [16/100], loss:0.0390
epoch [17/100], loss:0.0403
epoch [18/100], loss:0.0372
epoch [19/100], loss:0.0340
epoch [20/100], loss:0.0354
epoch [21/100], loss:0.0343
epoch [22/100], loss:0.0346
epoch [23/100], loss:0.0274
epoch [24/100], loss:0.0312
epoch [25/100], loss:0.0320
epoch [26/100], loss:0.0323
epoch [27/100], loss:0.0307
epoch [28/100], loss:0.0297
epoch [29/100], loss:0.0272
epoch [30/100], loss:0.0274
epoch [31/100], loss:0.0295
epoch [32/100], loss:0.0271
epoch [33/100], loss:0.0267
epoch [34/100], loss:0.0268
epoch [35/100], loss:0.0242
epoch [36/100], loss:0.0295
epoch [37/100], loss:0.0250
epoch [38/100], loss:0.0209
epoch [39/100], loss:0.0239
epoch [40/100], loss:0.0248
epoch [41/100], loss:0.0228
epoch [42/100], loss:0.0216
epoch [43/100], loss:0.0236
epoch [44/100], loss:0.0229
epoch [45/100], loss:0.0226
epoch [46/100], loss:0.0218
epoch [47/100], loss:0.0237
epoch [48/100], loss:0.0234
epoch [49/100], loss:0.0225
epoch [50/100], loss:0.0226
epoch [51/100], loss:0.0216
epoch [52/100], loss:0.0205
epoch [53/100], loss:0.0220
epoch [54/100], loss:0.0205
epoch [55/100], loss:0.0213
epoch [56/100], loss:0.0213
epoch [57/100], loss:0.0218
epoch [58/100], loss:0.0206
epoch [59/100], loss:0.0208
epoch [60/100], loss:0.0213
epoch [61/100], loss:0.0215
epoch [62/100], loss:0.0196
epoch [63/100], loss:0.0192
epoch [64/100], loss:0.0224
epoch [65/100], loss:0.0203
epoch [66/100], loss:0.0202
epoch [67/100], loss:0.0208
epoch [68/100], loss:0.0197
epoch [69/100], loss:0.0210
epoch [70/100], loss:0.0202
epoch [71/100], loss:0.0202
epoch [72/100], loss:0.0188
epoch [73/100], loss:0.0202
epoch [74/100], loss:0.0172
epoch [75/100], loss:0.0190
epoch [76/100], loss:0.0190
epoch [77/100], loss:0.0220
epoch [78/100], loss:0.0216
epoch [79/100], loss:0.0171
epoch [80/100], loss:0.0161
epoch [81/100], loss:0.0199
epoch [82/100], loss:0.0195
epoch [83/100], loss:0.0182
epoch [84/100], loss:0.0198
epoch [85/100], loss:0.0196
epoch [86/100], loss:0.0187
epoch [87/100], loss:0.0180
epoch [88/100], loss:0.0187
epoch [89/100], loss:0.0168
epoch [90/100], loss:0.0183
epoch [91/100], loss:0.0168
epoch [92/100], loss:0.0148
epoch [93/100], loss:0.0178
epoch [94/100], loss:0.0184
epoch [95/100], loss:0.0168
epoch [96/100], loss:0.0174
epoch [97/100], loss:0.0208
epoch [98/100], loss:0.0168
epoch [99/100], loss:0.0182
epoch [100/100], loss:0.0183
